{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a2aa92",
   "metadata": {},
   "source": [
    "# Model Tuning Action Plan\n",
    "- Top 3 models by balanced accuracy: \n",
    "    - Logistic regression (internal \"balanced\" about equal with manual oversample) 95% +/- 1%\n",
    "    - ExtraTrees classifier (subsample only, others not as good) 94.2% +/- 0.8%\n",
    "    - SVC (manual subsample or oversample, internal \"balanced\" not as good) 94% +/- 1.3%\n",
    "- Tune, then ROC/AUC for top 3\n",
    "- Compare to DummyClassifier\n",
    "- Run against test set for final validation\n",
    "- sklearn \"model stacking\" (predictions uncorrelated, maybe a weak with a strong? Different types, linear/not/etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af98dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and load data\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scaling, sampling, and modeling tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# basic models for testing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "df = pd.read_csv('data\\creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d97df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale \"amount\" and \"time\" features to match remaining\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "df['std_scaled_amount'] = std_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['std_scaled_time'] = std_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ec04a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data contains 344 fraudulent transactions which are 0.173% of all transactions\n",
      "Test Data contains 148 fraudulent transactions which are 0.173% of all transactions\n",
      "Original Data contains 492 fraudulent transactions which are 0.173% of all transactions\n"
     ]
    }
   ],
   "source": [
    "# train/test split BEFORE under or over sampling\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# split Class balance evaluation\n",
    "classcount = y_train.value_counts()\n",
    "perc_pos = (classcount[1]/(classcount[0]+classcount[1]))*100\n",
    "print(\"Training Data contains {} fraudulent transactions which are {:.3f}% of all transactions\".format(classcount[1], perc_pos))\n",
    "classcount = y_test.value_counts()\n",
    "perc_pos = (classcount[1]/(classcount[0]+classcount[1]))*100\n",
    "print(\"Test Data contains {} fraudulent transactions which are {:.3f}% of all transactions\".format(classcount[1], perc_pos))\n",
    "print(\"Original Data contains 492 fraudulent transactions which are 0.173% of all transactions\") # calculated in data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad1233c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9965942207085425"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run dummy classifier with \"stratified\" (ie predict according to class balance) strategy for comparison\n",
    "dummy = DummyClassifier(strategy='stratified', random_state=42)\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6591621b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.94860043        nan 0.95037468        nan 0.95007572\n",
      "        nan 0.95003803        nan 0.95002296        nan 0.95006567\n",
      "        nan 0.95007069]\n",
      "  warnings.warn(\n",
      "D:\\Documents\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the train scores are non-finite: [       nan 0.95385646        nan 0.95634024        nan 0.95714971\n",
      "        nan 0.95708313        nan 0.95666931        nan 0.9570712\n",
      "        nan 0.95672584]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "0.9503746812064537\n"
     ]
    }
   ],
   "source": [
    "# Tuning Logistic Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_values = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000], 'class_weight':['balanced']}\n",
    "logreg = LogisticRegression()\n",
    "tuned_lr = GridSearchCV(logreg, grid_values, scoring=\"balanced_accuracy\", return_train_score=True, n_jobs=-1)\n",
    "tuned_lr.fit(X_train, y_train)\n",
    "print(tuned_lr.best_params_)\n",
    "print(tuned_lr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a046642a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9287665383890251"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0149a0f",
   "metadata": {},
   "source": [
    "So, maybe 95% balanced accuracy is not so good? Or do I get to use \"balanced accuracy\" scoring on the dummy classifier? What's up with the non-finite scores warning? Also, tuning is basically no better than default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76c1ddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 10, 'min_samples_split': 6, 'n_estimators': 100, 'random_state': 42}\n",
      "0.94192242114237\n"
     ]
    }
   ],
   "source": [
    "# Tuning Extra Trees with subsample\n",
    "subsample = RandomUnderSampler(random_state=42)\n",
    "X_tr_sub, y_tr_sub = subsample.fit_resample(X_train, y_train) #grid search didn't work as a pipeline\n",
    "extree = ExtraTreesClassifier() \n",
    "grid_values = {'n_estimators': [10, 100, 500, 1000], 'max_features': [1, 2, 5, 10, 'sqrt', 'log2'],\n",
    "               'min_samples_split': [2, 4, 6, 8, 10], 'random_state': [42]}\n",
    "tuned_ext = GridSearchCV(extree, grid_values, scoring=\"balanced_accuracy\", return_train_score=True, n_jobs=-1)\n",
    "tuned_ext.fit(X_tr_sub, y_tr_sub)\n",
    "print(tuned_ext.best_params_)\n",
    "print(tuned_ext.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff8ce04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.918471267445416"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_ext.score(X_test, y_test) # assuming I don't apply subsampling to test data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e09a49",
   "metadata": {},
   "source": [
    "Again, tuning is no better than defaults; running against test holdout is worse; and scores don't approach the dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "749f95a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'gamma': 0.1, 'kernel': 'linear', 'random_state': 42}\n",
      "0.9447996589940324\n"
     ]
    }
   ],
   "source": [
    "# Tuning SVC with subsample\n",
    "svc = SVC()\n",
    "grid_values = {'kernel': ['linear', 'rbf', 'poly'], 'gamma': [0.1, 1, 10, 100],\n",
    "               'C': [0.1, 1, 10, 100, 1000], 'random_state': [42]}\n",
    "tuned_svc = GridSearchCV(svc, grid_values, scoring=\"balanced_accuracy\", return_train_score=True, n_jobs=-1)\n",
    "tuned_svc.fit(X_tr_sub, y_tr_sub)\n",
    "print(tuned_svc.best_params_)\n",
    "print(tuned_svc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d57115ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9300463177873928"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6536dc",
   "metadata": {},
   "source": [
    "Well, this one produced a ~0.005 improvement and its test score is at least closer to its train score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbc46daa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'RocCurveDisplay' has no attribute 'from_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-22147972ddcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRocCurveDisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mRocCurveDisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuned_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'RocCurveDisplay' has no attribute 'from_estimator'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "RocCurveDisplay.from_estimator(tuned_lr.best_estimator_, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a1a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
