{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thinking out loud, at the beginning:\n",
    "- Wow these files are big, I'm going to need to pare down the data probably a lot.\n",
    "- First going to be looking for \"what's the most complete data that I have for what span of time\"?\n",
    "- What data do I actually want to focus on? (record some research on how, broadly, one comes up with a company valuation that could be comparable to market capitalization)\n",
    "- I probably should focus my market data gathering on the day after each SEC filing is released, assuming that will be the day that stock price movements are most influenced by the filing information rather than other news or factors.\n",
    "- My core question so far is, are market prices correlated with valuation information in some fashion?\n",
    "    - If so, which valuation information is most strongly correlated?\n",
    "    - What, if any, is the relationship between stock price/market capitalization and the business's balance sheet?\n",
    "    - Do any of these data include attempts to put a value on intangibles such as goodwill/reputation?\n",
    "    - The fundamental analysis tradition focuses on ratios of price to earnings, book value, earnings growth; do the ratios that investors find acceptable change over time? Do they tend to change back (or ahead) when SEC filings are released?\n",
    "    - (Do I have time for this?) What happens in the periods between filings? Do prices maintain a range, like the moving average support/resistance bands favored by technical analysis?\n",
    "- Therefore, what am I looking for in the data I've found so far?\n",
    "    - As much history as I can assemble from both market data and SEC filings; will need to drop years where I have one but not the other.\n",
    "    - Focus on the Fortune 100, probably choose the 100 as of the most recent year I have data for.\n",
    "    - Market data at daily granularity, hopefully with open, high, low, and close prices for each day. Probably focus on dates related to the company's SEC filings.\n",
    "    - Still deciding what SEC filing information, partially I don't know what I have to work with yet, hoping for a moderately quick thumbnail of value related to assets, liabilities, and maybe intangibles/goodwill. Assuming earnings-per-share numbers also will be found here.\n",
    "    - If market data has already calculated P/E or other ratios, great, I can use those to sanity-check what I'm pulling in from SEC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First crack at a plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Open each dataset (I have 4 right now) individually, probably not in code. \n",
    "- Need to see file structure and years covered. Make decision on how much history to collect/collate.\n",
    "    - Market datasets from:\n",
    "        - https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs\n",
    "            - description does not offer a begin date but last updated in 2017-Nov\n",
    "            - data appears to be in txt files by ticker: file for AAPL dates to 1984-Sep and columns are Date,Open,High,Low,Close,Volume,OpenInt\n",
    "            - there is a comment that prices in this data are adjusted for dividends AND splits (as opposed to Yahoo Finance data, which is not or not always--not sure what that means)\n",
    "        - https://www.kaggle.com/tsaustin/us-historical-stock-prices-with-earnings-data\n",
    "            - 20 years according to description; last updated 2020-Dec\n",
    "            - Data in 4 files: stock price, earnings (estimated and reported), dividends, and a summary\n",
    "    - SEC datasets from:\n",
    "        - https://www.kaggle.com/miguelaenlle/parsed-sec-10q-filings-since-2006\n",
    "            - Since 2006 according to description; Last updated 2020-Jul\n",
    "            - Notes that ~50% of data is null, which might be 'not found in filing' or 'not found by parser'.\n",
    "            - Does have lots of promising columns like commonstocksharesissued, assetscurrent, accountspayablecurrent, commonstockvalue, liabilities, liabilitiesandstockholdersequity, stockholdersequity, earningspersharebasic, netincomeloss, profitloss, costofgoodssold, filing_date, costsandexpenses, cash \n",
    "        - https://www.kaggle.com/finnhub/reported-financials\n",
    "            - 2010-2020, according to description. Last updated 2021-Mar\n",
    "            - 4 GB data!\n",
    "            - Data is in folders by year-qtr; JSON files. Will take some work, I think, to find the tickers I want.\n",
    "            \n",
    "So, I can probably get 2010-2020 reasonably complete. Maybe 2006-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WMT', 'AMZN', 'XOM', 'AAPL', 'CVS', 'BRKA', 'UNH', 'MCK', 'T', 'ABC', 'GOOGL', 'F', 'CI', 'COST', 'CVX', 'CAH', 'JPM', 'GM', 'WBA', 'VZ', 'MSFT', 'MPC', 'KR', 'FNMA', 'BAC', 'HD', 'PSX', 'CMCS.A', 'ANTM', 'WFC', 'C', 'VLO', 'GE', 'DELL', 'JNJ', 'TGT', 'IBM', 'RTX', 'BA', 'FMCC', 'CNC', 'UPS', 'LOW', 'INTC', 'FB', 'FDX', 'MET', 'DIS', 'PG', 'PEP', 'HUM', 'PRU', 'ADM', 'ACI', 'SYY', 'LMT', 'HPQ', 'ET', 'GS', 'MS', 'CAT', 'CSCO', 'PFE', 'HCA', 'AIG', 'AXP', 'DAL', 'MRK', 'AAL', 'CHTR', 'ALL', 'BBY', 'UAL', 'DOW', 'TSN', 'TJX', 'ORCL', 'GD', 'DE', 'NKE', 'PGR', 'KO', 'TECD', 'INT', 'HON', 'COP', 'EXC', 'NOC', 'COF', 'PAGP', 'ABBV', 'SNEX']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# lots of code will depend on this ticker list\n",
    "# csv-to-list found at https://stackoverflow.com/questions/21065938/read-a-single-column-of-a-csv-and-store-in-an-array\n",
    "ticker_list = pd.read_csv('Fortune100.csv', usecols=['Ticker'], na_values='-').dropna().T.values.tolist()[0]\n",
    "print(ticker_list)\n",
    "#ticker_list = ['AAPL'] used for original code creation to reduce complexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Open each dataset in code and examine columns.\n",
    "See if I can choose one market and one SEC dataset; do they contain the same information? Is there just a column or two from one I can add to the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a: the Finnhub SEC filings dataset\n",
    "- Hopefully the most complicated one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: 3726\n",
      "Problem files found: 1\n"
     ]
    }
   ],
   "source": [
    "# Note some of this code may not work as written except on my own computer, or if you download the datasets like I did.\n",
    "# Not planning to upload all this data, it's waaay too much.\n",
    "# CAUTION: walking all this data will take a long time! Over 200K files, expect 20+ min of \"busy\" before you get an answer.\n",
    "# I am grateful that Jupyter changes the tab icon to hourglass when a notebook is busy!\n",
    "\n",
    "# For Finnhub reported-financials dataset:\n",
    "# Recurse through many YYYY.Q# subdirectories using os.walk\n",
    "# Acquire list of files containing the ticker(s) I want (define a list somewhere early!)\n",
    "# Copy those files to a \"data I'm keeping\" folder\n",
    "# Import reduced file set into a dataframe for analysis\n",
    "# I thought about defining a function, but it's pretty specific to how this one dataset is laid out, is it worth it?\n",
    "\n",
    "\n",
    "start_dir = os.path.join(os.getcwd(), 'SEC-FinancialsAsReported') \n",
    "files_list = []\n",
    "problem_files = []\n",
    "\n",
    "for root, subdirs, files in os.walk(start_dir):\n",
    "    for filename in files:\n",
    "        if os.path.splitext(filename)[1] == ('.json'):\n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                df = pd.read_json(file_path) # is there a better way to read json if all I want is to check one column?\n",
    "            except ValueError as val_err:\n",
    "                problem_files.append((file_path, \"Value Error: {0}\".format(val_err)))\n",
    "            except:\n",
    "                print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "                raise\n",
    "            else:\n",
    "                set_tickers = set(ticker_list)\n",
    "                set_symbols = set(df['symbol'].unique())\n",
    "                if set_tickers.intersection(set_symbols):\n",
    "                    # print('Found one!')\n",
    "                    files_list.append(file_path)\n",
    "\n",
    "print('Files found: ' + str(len(files_list)))\n",
    "print('Problem files found: ' + str(len(problem_files)))\n",
    "# Files found with original single ticker: 44, 1 problem file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### experimenting with the one problem file. Set lines? orient? skip because it's one file out of 200K or so?\n",
    "`\n",
    "print(os.getcwd())  \n",
    "file_problem = os.path.join(os.getcwd(),'SEC-FinancialsAsReported\\\\2016.QTR4\\\\0001098009-16-000023.json')  \n",
    "print(file_problem)  \n",
    "`\n",
    "\n",
    "tried a lot of variations to get path join to work by hand (as opposed to using walk). \n",
    "The two most important seem to be start with os.getcwd() and escape any slashes in the path.  \n",
    "`\n",
    "df = pd.read_json(file_problem) #as done in filewalk above, \"value is too big\"  \n",
    "df = pd.read_json(file_problem, lines=True) #did not work, \"expected object or value\"  \n",
    "df = pd.read_json(file_problem, orient='records') #did not work, \"value is too big\"  \n",
    "df = pd.read_json(file_problem, orient='split') #did not work, \"value is too big\"  \n",
    "df = pd.read_json(file_problem, orient='index') #did not work, \"value is too big\"  \n",
    "df = pd.read_json(file_problem, orient='columns') #did not work, \"value is too big\"  \n",
    "df = pd.read_json(file_problem, orient='values') #did not work, \"value is too big\"  \n",
    "print(df.head)  \n",
    "`  \n",
    "nothing is working so far, skip this damn file. Opened in Notepad++ and its ticker is 'CWNM' \n",
    "which I don't think is on my list anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, to see what I have for my ticker(s) in finnhub files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                concept unit       value   startDate  \\\n",
      "0        CashAndCashEquivalentsAtCarryingValue  usd  1614607000  2009-02-01   \n",
      "1                         ShortTermInvestments  usd   130636000  2009-02-01   \n",
      "2                 AccountsReceivableNetCurrent  usd   148126000  2009-02-01   \n",
      "3                                 InventoryNet  usd  2532318000  2009-02-01   \n",
      "4                           OtherAssetsCurrent  usd   255707000  2009-02-01   \n",
      "\n",
      "      endDate  year quarter symbol  \n",
      "0  2010-01-30  2010      FY    TJX  \n",
      "1  2010-01-30  2010      FY    TJX  \n",
      "2  2010-01-30  2010      FY    TJX  \n",
      "3  2010-01-30  2010      FY    TJX  \n",
      "4  2010-01-30  2010      FY    TJX  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 123738 entries, 0 to 123737\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   label      123738 non-null  object\n",
      " 1   concept    123738 non-null  object\n",
      " 2   unit       123738 non-null  object\n",
      " 3   value      123738 non-null  object\n",
      " 4   startDate  123738 non-null  object\n",
      " 5   endDate    123738 non-null  object\n",
      " 6   year       123738 non-null  object\n",
      " 7   quarter    123738 non-null  object\n",
      " 8   symbol     123738 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 8.5+ MB\n",
      "None\n",
      "         label                           concept    unit   value   startDate  \\\n",
      "count   123738                            123738  123738  123738      123738   \n",
      "unique   10160                              1140       4   54834         205   \n",
      "top             LiabilitiesAndStockholdersEquity     usd     N/A  2017-01-01   \n",
      "freq     19647                              3672  123532    2606        8527   \n",
      "\n",
      "           endDate    year quarter  symbol  \n",
      "count       123738  123738  123738  123738  \n",
      "unique         534      13       5      88  \n",
      "top     2016-12-31    2019      Q2     EXC  \n",
      "freq          2400   12091   31126    2563  \n",
      "['2010-01-30' '2009-12-31' '2010-03-31' '2010-03-27' '2010-04-04'\n",
      " '2010-05-01' '2010-04-30' '2010-02-27' '2010-04-02' '2010-05-22'\n",
      " '2010-03-28' '2010-03-20' '2010-04-03' '2010-05-02' '2010-05-09'\n",
      " '2010-06-30' '2010-07-30' '2010-06-26' '2010-07-04' '2010-07-31'\n",
      " '2010-07-03' '2010-07-02' '2010-05-29' '2010-08-14' '2010-05-31'\n",
      " '2010-06-12' '2010-06-27' '2010-08-01' '2010-09-30' '2010-10-29'\n",
      " '2010-09-25' '2010-10-02' '2010-10-03' '2010-10-30' '2010-10-01'\n",
      " '2010-10-31' '2010-08-28' '2010-11-06' '2010-08-31' '2010-09-04'\n",
      " '2010-08-29' '2010-09-26' '2010-11-21' '2010-12-31' '2011-01-28'\n",
      " '2011-01-01' '2010-12-25' '2011-01-02' '2011-02-28' '2011-01-29'\n",
      " '2011-01-31' '2010-11-27' '2010-11-30' '2011-02-13' '2011-01-30'\n",
      " '2011-03-31' '2011-04-29' '2011-04-02' '2011-04-03' '2011-04-30'\n",
      " '2011-02-26' '2011-04-01' '2011-05-21' '2011-03-26' '2011-03-27'\n",
      " '2011-03-19' '2011-05-01' '2011-05-08' '2011-05-31' '2011-06-30'\n",
      " '2011-07-29' '2011-08-27' '2011-07-02' '2011-07-03' '2011-07-30'\n",
      " '2011-08-31' '2011-07-01' '2011-07-31' '2011-05-28' '2011-08-13'\n",
      " '2011-06-25' '2011-06-11' '2011-06-26' '2011-09-30' '2011-10-28'\n",
      " '2011-10-30' '2011-10-01' '2011-10-02' '2011-10-29' '2011-11-30'\n",
      " '2011-10-31' '2011-11-05' '2011-09-03' '2011-08-28' '2011-09-24'\n",
      " '2011-09-25' '2011-11-20' '2011-12-31' '2012-01-29' '2011-11-26'\n",
      " '2012-02-03' '2012-01-31' '2012-01-28' '2012-01-01' '2012-02-12'\n",
      " '2012-02-29' '2012-03-31' '2012-03-30' '2012-05-04' '2012-04-01'\n",
      " '2012-04-29' '2012-03-03' '2012-05-05' '2012-04-30' '2012-04-28'\n",
      " '2012-05-19' '2012-03-24' '2012-03-25' '2012-05-06' '2012-05-31'\n",
      " '2012-06-30' '2012-06-29' '2012-08-03' '2012-07-31' '2012-07-01'\n",
      " '2012-07-29' '2012-08-04' '2012-07-28' '2012-08-11' '2012-06-24'\n",
      " '2012-06-16' '2012-08-31' '2012-09-30' '2012-09-28' '2012-11-02'\n",
      " '2012-09-29' '2012-10-31' '2012-10-28' '2012-11-03' '2012-10-27'\n",
      " '2012-09-02' '2012-11-30' '2012-11-25' '2012-09-08' '2012-12-31'\n",
      " '2012-12-29' '2013-01-31' '2012-12-30' '2013-02-03' '2013-02-01'\n",
      " '2013-01-26' '2013-02-02' '2013-02-17' '2013-02-28' '2013-03-31'\n",
      " '2013-03-29' '2013-03-30' '2013-05-03' '2013-03-23' '2013-04-30'\n",
      " '2013-05-05' '2013-05-04' '2013-04-27' '2013-05-25' '2013-05-31'\n",
      " '2013-05-12' '2013-06-30' '2013-06-28' '2013-06-29' '2013-08-02'\n",
      " '2013-06-15' '2013-07-31' '2013-08-04' '2013-08-03' '2013-07-27'\n",
      " '2013-08-17' '2013-08-31' '2013-09-30' '2013-09-27' '2013-11-02'\n",
      " '2013-09-29' '2013-09-28' '2013-11-01' '2013-09-07' '2013-10-31'\n",
      " '2013-11-03' '2013-10-26' '2013-11-24' '2013-11-09' '2013-11-30'\n",
      " '2013-09-01' '2013-12-31' '2014-02-01' '2013-12-28' '2014-01-31'\n",
      " '2013-12-29' '2014-02-02' '2014-01-25' '2014-02-16' '2014-02-28'\n",
      " '2014-03-31' '2014-03-28' '2014-05-03' '2014-03-30' '2014-03-29'\n",
      " '2014-05-02' '2014-03-22' '2014-04-30' '2014-05-04' '2014-04-26'\n",
      " '2014-05-11' '2014-05-31' '2014-06-30' '2014-06-27' '2014-08-02'\n",
      " '2014-06-29' '2014-06-28' '2014-08-01' '2014-06-14' '2014-07-31'\n",
      " '2014-08-03' '2014-07-26' '2014-05-24' '2014-08-16' '2014-08-31'\n",
      " '2014-09-30' '2014-09-26' '2014-11-01' '2014-09-28' '2014-09-27'\n",
      " '2014-10-31' '2014-09-06' '2014-11-02' '2014-10-25' '2014-11-23'\n",
      " '2014-11-08' '2014-11-30' '2014-12-31' '2015-01-31' '2014-12-27'\n",
      " '2015-01-30' '2014-12-28' '2015-02-01' '2015-01-24' '2015-02-15'\n",
      " '2015-02-28' '2015-03-31' '2015-04-03' '2015-05-02' '2015-04-05'\n",
      " '2015-03-28' '2015-05-01' '2015-03-21' '2015-03-29' '2015-04-30'\n",
      " '2015-05-03' '2015-04-25' '2015-05-10' '2015-05-23' '2015-05-31'\n",
      " '2015-06-30' '2015-07-03' '2015-08-01' '2015-07-05' '2015-06-27'\n",
      " '2015-07-31' '2015-06-13' '2015-06-28' '2015-08-02' '2015-07-25'\n",
      " '2015-08-15' '2015-08-31' '2015-09-30' '2015-10-02' '2015-10-31'\n",
      " '2015-10-04' '2015-09-26' '2015-10-30' '2015-09-05' '2015-09-27'\n",
      " '2015-10-03' '2015-11-01' '2015-10-24' '2015-08-30' '2015-11-22'\n",
      " '2015-11-07' '2015-11-30' '2015-12-31' '2016-01-30' '2015-12-26'\n",
      " '2016-01-29' '2016-01-02' '2016-01-31' '2016-01-03' '2016-01-23'\n",
      " '2016-02-14' '2016-02-29' '2016-03-31' '2016-04-01' '2016-04-30'\n",
      " '2016-04-03' '2016-04-02' '2016-04-29' '2016-03-19' '2016-03-26'\n",
      " '2016-05-01' '2016-05-08' '2016-05-21' '2016-03-27' '2016-05-31'\n",
      " '2016-06-30' '2016-07-01' '2016-07-30' '2016-07-03' '2016-07-31'\n",
      " '2016-07-02' '2016-07-29' '2016-06-11' '2016-09-03' '2016-08-13'\n",
      " '2016-06-26' '2016-08-31' '2016-06-25' '2016-09-30' '2016-10-29'\n",
      " '2016-10-02' '2016-10-31' '2016-10-01' '2016-10-28' '2016-10-30'\n",
      " '2016-08-28' '2016-11-20' '2016-09-25' '2016-11-30' '2016-11-05'\n",
      " '2016-09-24' '2016-12-31' '2017-01-28' '2017-01-31' '2017-01-01'\n",
      " '2017-01-29' '2017-02-12' '2017-02-28' '2017-03-31' '2017-04-29'\n",
      " '2017-04-02' '2017-04-30' '2017-04-01' '2017-02-03' '2017-05-05'\n",
      " '2017-03-25' '2017-05-07' '2017-03-26' '2017-05-31' '2017-05-20'\n",
      " '2017-06-30' '2017-07-29' '2017-07-02' '2017-07-31' '2017-07-01'\n",
      " '2017-08-04' '2017-06-17' '2017-07-30' '2017-06-25' '2017-08-31'\n",
      " '2017-08-12' '2017-09-30' '2017-09-29' '2017-10-28' '2017-10-01'\n",
      " '2017-10-31' '2017-11-03' '2017-09-09' '2017-10-29' '2017-09-03'\n",
      " '2017-11-26' '2017-09-24' '2017-11-30' '2017-11-04' '2017-12-31'\n",
      " '2018-02-03' '2018-01-31' '2017-12-30' '2018-01-28' '2018-01-27'\n",
      " '2018-02-18' '2018-02-28' '2018-03-31' '2018-03-30' '2018-05-05'\n",
      " '2018-04-01' '2018-04-30' '2018-02-02' '2018-05-04' '2018-03-24'\n",
      " '2018-04-29' '2018-04-28' '2018-05-13' '2018-03-25' '2018-05-31'\n",
      " '2018-05-26' '2018-02-24' '2018-06-30' '2018-06-29' '2018-08-04'\n",
      " '2018-07-01' '2018-07-31' '2018-08-03' '2018-06-16' '2018-07-29'\n",
      " '2018-07-28' '2018-06-24' '2018-08-18' '2018-08-31' '2018-09-30'\n",
      " '2018-09-28' '2018-11-03' '2018-10-31' '2018-09-29' '2018-11-02'\n",
      " '2018-09-08' '2018-10-28' '2018-10-27' '2018-09-02' '2018-11-25'\n",
      " '2018-11-10' '2018-11-30' '2018-12-31' '2019-02-02' '2019-01-31'\n",
      " '2018-12-29' '2018-12-30' '2019-02-03' '2019-01-26' '2019-02-17'\n",
      " '2019-01-27' '2019-02-28' '2019-02-01' '2018-12-01' '2019-03-31'\n",
      " '2019-03-29' '2019-05-04' '2019-04-30' '2019-03-30' '2019-05-03'\n",
      " '2019-03-23' '2019-05-05' '2019-04-27' '2019-05-12' '2019-04-28'\n",
      " '2019-05-25' '2019-05-31' '2019-02-23' '2019-06-30' '2019-06-28'\n",
      " '2019-08-03' '2019-07-31' '2019-06-29' '2019-08-02' '2019-06-15'\n",
      " '2019-08-04' '2019-07-27' '2019-07-28' '2019-08-17' '2019-08-31'\n",
      " '2019-09-30' '2019-09-27' '2019-11-02' '2019-09-29' '2019-10-31'\n",
      " '2019-09-28' '2019-11-01' '2019-09-07' '2019-11-03' '2019-10-26'\n",
      " '2019-09-01' '2019-11-24' '2019-11-09' '2019-11-30' '2019-12-31'\n",
      " '2020-02-01' '2020-01-31' '2019-12-28' '2019-12-29' '2020-02-02'\n",
      " '2020-01-25' '2020-02-16' '2020-02-29' '2020-03-31' '2020-03-27'\n",
      " '2020-05-02' '2020-03-29' '2020-04-30' '2020-03-28' '2020-05-01'\n",
      " '2020-03-21' '2020-05-03' '2020-04-25' '2020-05-10' '2020-05-23'\n",
      " '2020-05-31' '2020-06-30' '2020-06-26' '2020-08-01' '2020-06-28'\n",
      " '2020-07-31' '2020-06-27' '2020-06-13' '2020-08-31' '2020-08-02'\n",
      " '2020-07-25' '2020-08-15' '2020-06-20' '2020-09-30' '2020-09-25'\n",
      " '2020-10-31' '2020-09-27' '2020-09-26' '2020-10-30' '2020-09-05'\n",
      " '2020-10-03' '2020-11-01' '2020-10-24' '2020-08-30' '2020-11-22'\n",
      " '2020-11-07' '2020-11-30' '2020-09-12' '2020-12-31' '2020-12-26'\n",
      " '2021-01-02' '2021-01-03' '2021-01-31' '2020-12-05']\n",
      "['TJX' 'MRK' 'KR' 'ADM' 'CAT' 'DOW' 'GE' 'T' 'HON' 'NOC' 'INTC' 'SYY'\n",
      " 'MCK' 'COP' 'CVX' 'MET' 'CI' 'VLO' 'ABC' 'WFC' 'GS' 'ACI' 'JPM' 'JNJ'\n",
      " 'DELL' 'BBY' 'KO' 'AIG' 'C' 'HPQ' 'IBM' 'ALL' 'DE' 'TGT' 'BA' 'LMT'\n",
      " 'AAPL' 'MSFT' 'AMZN' 'PEP' 'HUM' 'VZ' 'PG' 'GD' 'CVS' 'UNH' 'AXP' 'CAH'\n",
      " 'XOM' 'BAC' 'COF' 'UPS' 'MS' 'DIS' 'CSCO' 'HD' 'WMT' 'COST' 'LOW' 'EXC'\n",
      " 'DAL' 'CNC' 'ORCL' 'NKE' 'INT' 'TSN' 'TECD' 'FDX' 'UAL' 'PRU' 'HCA' 'F'\n",
      " 'FMCC' 'BRKA' 'PFE' 'GM' 'MPC' 'CHTR' 'PSX' 'FB' 'ET' 'ABBV' 'PGR' 'PAGP'\n",
      " 'ANTM' 'AAL' 'WBA' 'RTX']\n"
     ]
    }
   ],
   "source": [
    "# json_normalize found at https://stackoverflow.com/questions/20638006/convert-list-of-dictionaries-to-a-pandas-dataframe\n",
    "# convert to dict clue found at https://stackoverflow.com/questions/55710154/python-json-normalize-string-indices-must-be-integers-error\n",
    "# nested JSON clue 1: https://stackoverflow.com/questions/47341519/json-normalize-for-dicts-within-dicts\n",
    "# nested JSON clue 2: https://stackoverflow.com/questions/46091362/how-to-normalize-json-correctly-by-python-pandas (needs json.load)\n",
    "# nested JSON clue 3: https://www.kaggle.com/jboysen/quick-tutorial-flatten-nested-json-in-pandas/notebook\n",
    "bs_df_list = []\n",
    "cf_df_list = []\n",
    "ic_df_list = []\n",
    "\n",
    "for i in range(len(files_list)):\n",
    "    with open(files_list[i]) as f:\n",
    "        d = json.load(f)\n",
    "    balsheet_df = pd.json_normalize(d, record_path=['data','bs'], meta=['startDate','endDate','year','quarter','symbol'])\n",
    "    cashflow_df = pd.json_normalize(d, record_path=['data','cf'], meta=['startDate','endDate','year','quarter','symbol'])\n",
    "    income_df = pd.json_normalize(d, record_path=['data','ic'], meta=['startDate','endDate','year','quarter','symbol'])\n",
    "    bs_df_list.append(balsheet_df)\n",
    "    cf_df_list.append(cashflow_df)\n",
    "    ic_df_list.append(income_df)\n",
    "    \n",
    "#print(bs_df_list)\n",
    "bs_df = pd.concat(bs_df_list, ignore_index=True)\n",
    "cf_df = pd.concat(cf_df_list, ignore_index=True)\n",
    "ic_df = pd.concat(ic_df_list, ignore_index=True)\n",
    "\n",
    "print(bs_df.head())\n",
    "print(bs_df.info())\n",
    "print(bs_df.describe())\n",
    "print(bs_df.endDate.unique())\n",
    "print(bs_df.symbol.unique())\n",
    "# These 3 dataframes with single ticker held ~1000-1300 records over 44 quarterly filings and occupied 70-90K memory each\n",
    "# One ticker likely to not be enough data points for good machine learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               label  \\\n",
      "0                                                      \n",
      "1                                                      \n",
      "2  Loss on property disposals and impairment charges   \n",
      "3                                                      \n",
      "4                                                      \n",
      "\n",
      "                                           concept unit       value  \\\n",
      "0                                       ProfitLoss  usd  1213572000   \n",
      "1             DepreciationDepletionAndAmortization  usd   435218000   \n",
      "2  tjx:LossOnPropertyDisposalsAndImpairmentCharges  usd    10270000   \n",
      "3                  DeferredIncomeTaxExpenseBenefit  usd    53155000   \n",
      "4                           ShareBasedCompensation  usd    55145000   \n",
      "\n",
      "    startDate     endDate  year quarter symbol  \n",
      "0  2009-02-01  2010-01-30  2010      FY    TJX  \n",
      "1  2009-02-01  2010-01-30  2010      FY    TJX  \n",
      "2  2009-02-01  2010-01-30  2010      FY    TJX  \n",
      "3  2009-02-01  2010-01-30  2010      FY    TJX  \n",
      "4  2009-02-01  2010-01-30  2010      FY    TJX  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 121696 entries, 0 to 121695\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   label      121696 non-null  object\n",
      " 1   concept    121696 non-null  object\n",
      " 2   unit       121696 non-null  object\n",
      " 3   value      121696 non-null  object\n",
      " 4   startDate  121696 non-null  object\n",
      " 5   endDate    121696 non-null  object\n",
      " 6   year       121696 non-null  object\n",
      " 7   quarter    121696 non-null  object\n",
      " 8   symbol     121696 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 8.4+ MB\n",
      "None\n",
      "         label                                     concept    unit   value  \\\n",
      "count   121696                                      121696  121696  121696   \n",
      "unique    7414                                        2001       4   31763   \n",
      "top             NetCashProvidedByUsedInFinancingActivities     usd       0   \n",
      "freq     18819                                        2935  121690    3973   \n",
      "\n",
      "         startDate     endDate    year quarter  symbol  \n",
      "count       121696      121696  121696  121696  121696  \n",
      "unique         204         534      13       5      88  \n",
      "top     2017-01-01  2016-12-31    2016      FY     ALL  \n",
      "freq          8403        2661   12005   32992    2370  \n",
      "['2010-01-30' '2009-12-31' '2010-03-31' '2010-04-30' '2010-03-27'\n",
      " '2010-04-04' '2010-05-01' '2010-02-27' '2010-04-02' '2010-05-22'\n",
      " '2010-03-28' '2010-03-20' '2010-04-03' '2010-05-02' '2010-05-09'\n",
      " '2010-06-30' '2010-07-30' '2010-06-26' '2010-07-04' '2010-07-31'\n",
      " '2010-07-03' '2010-07-02' '2010-05-29' '2010-08-14' '2010-05-31'\n",
      " '2010-06-12' '2010-06-27' '2010-08-01' '2010-09-30' '2010-10-29'\n",
      " '2010-09-25' '2010-10-02' '2010-10-03' '2010-10-30' '2010-10-01'\n",
      " '2010-10-31' '2010-08-28' '2010-11-06' '2010-08-31' '2010-09-04'\n",
      " '2010-08-29' '2010-09-26' '2010-11-21' '2010-12-31' '2011-01-28'\n",
      " '2011-01-01' '2010-12-25' '2011-01-02' '2011-02-28' '2011-01-29'\n",
      " '2011-01-31' '2010-11-27' '2010-11-30' '2011-02-13' '2011-01-30'\n",
      " '2011-03-31' '2011-04-29' '2011-04-02' '2011-04-03' '2011-04-30'\n",
      " '2011-02-26' '2011-04-01' '2011-05-21' '2011-03-26' '2011-03-27'\n",
      " '2011-03-19' '2011-05-01' '2011-05-08' '2011-05-31' '2011-06-30'\n",
      " '2011-07-29' '2011-08-27' '2011-07-02' '2011-07-03' '2011-07-30'\n",
      " '2011-08-31' '2011-07-01' '2011-07-31' '2011-05-28' '2011-08-13'\n",
      " '2011-06-25' '2011-06-11' '2011-06-26' '2011-09-30' '2011-10-28'\n",
      " '2011-10-30' '2011-10-01' '2011-10-02' '2011-10-29' '2011-11-30'\n",
      " '2011-10-31' '2011-11-05' '2011-09-03' '2011-08-28' '2011-09-24'\n",
      " '2011-09-25' '2011-11-20' '2011-12-31' '2012-01-29' '2011-11-26'\n",
      " '2012-02-03' '2012-01-31' '2012-01-28' '2012-01-01' '2012-02-12'\n",
      " '2012-02-29' '2012-03-31' '2012-03-30' '2012-05-04' '2012-04-01'\n",
      " '2012-04-29' '2012-03-03' '2012-05-05' '2012-04-30' '2012-04-28'\n",
      " '2012-05-19' '2012-03-24' '2012-03-25' '2012-05-06' '2012-05-31'\n",
      " '2012-06-30' '2012-06-29' '2012-08-03' '2012-07-31' '2012-07-01'\n",
      " '2012-07-29' '2012-08-04' '2012-07-28' '2012-08-11' '2012-06-24'\n",
      " '2012-06-16' '2012-08-31' '2012-09-30' '2012-09-28' '2012-11-02'\n",
      " '2012-09-29' '2012-10-31' '2012-10-28' '2012-11-03' '2012-10-27'\n",
      " '2012-09-02' '2012-11-30' '2012-11-25' '2012-09-08' '2012-12-31'\n",
      " '2012-12-29' '2013-01-31' '2012-12-30' '2013-02-03' '2013-02-01'\n",
      " '2013-01-26' '2013-02-02' '2013-02-17' '2013-02-28' '2013-03-31'\n",
      " '2013-03-29' '2013-03-30' '2013-05-03' '2013-03-23' '2013-04-30'\n",
      " '2013-05-05' '2013-05-04' '2013-04-27' '2013-05-25' '2013-05-31'\n",
      " '2013-05-12' '2013-06-30' '2013-06-28' '2013-06-29' '2013-08-02'\n",
      " '2013-06-15' '2013-07-31' '2013-08-04' '2013-08-03' '2013-07-27'\n",
      " '2013-08-17' '2013-08-31' '2013-09-30' '2013-09-27' '2013-11-02'\n",
      " '2013-09-29' '2013-09-28' '2013-11-01' '2013-09-07' '2013-10-31'\n",
      " '2013-11-03' '2013-10-26' '2013-11-24' '2013-11-09' '2013-11-30'\n",
      " '2013-09-01' '2013-12-31' '2014-02-01' '2013-12-28' '2014-01-31'\n",
      " '2013-12-29' '2014-02-02' '2014-01-25' '2014-02-16' '2014-02-28'\n",
      " '2014-03-31' '2014-03-28' '2014-05-03' '2014-03-30' '2014-03-29'\n",
      " '2014-05-02' '2014-03-22' '2014-04-30' '2014-05-04' '2014-04-26'\n",
      " '2014-05-11' '2014-05-31' '2014-06-30' '2014-06-27' '2014-08-02'\n",
      " '2014-06-29' '2014-06-28' '2014-08-01' '2014-06-14' '2014-07-31'\n",
      " '2014-08-03' '2014-07-26' '2014-05-24' '2014-08-16' '2014-08-31'\n",
      " '2014-09-30' '2014-09-26' '2014-11-01' '2014-09-28' '2014-09-27'\n",
      " '2014-10-31' '2014-09-06' '2014-11-02' '2014-10-25' '2014-11-23'\n",
      " '2014-11-08' '2014-11-30' '2014-12-31' '2015-01-31' '2014-12-27'\n",
      " '2015-01-30' '2014-12-28' '2015-02-01' '2015-01-24' '2015-02-15'\n",
      " '2015-02-28' '2015-03-31' '2015-04-03' '2015-05-02' '2015-04-05'\n",
      " '2015-03-28' '2015-05-01' '2015-03-21' '2015-03-29' '2015-04-30'\n",
      " '2015-05-03' '2015-04-25' '2015-05-10' '2015-05-23' '2015-05-31'\n",
      " '2015-06-30' '2015-07-03' '2015-08-01' '2015-07-05' '2015-06-27'\n",
      " '2015-07-31' '2015-06-13' '2015-06-28' '2015-08-02' '2015-07-25'\n",
      " '2015-08-15' '2015-08-31' '2015-09-30' '2015-10-02' '2015-10-31'\n",
      " '2015-10-04' '2015-09-26' '2015-10-30' '2015-09-05' '2015-09-27'\n",
      " '2015-10-03' '2015-11-01' '2015-10-24' '2015-08-30' '2015-11-22'\n",
      " '2015-11-07' '2015-11-30' '2015-12-31' '2016-01-30' '2015-12-26'\n",
      " '2016-01-29' '2016-01-02' '2016-01-31' '2016-01-03' '2016-01-23'\n",
      " '2016-02-14' '2016-02-29' '2016-03-31' '2016-04-01' '2016-04-30'\n",
      " '2016-04-03' '2016-04-02' '2016-04-29' '2016-03-19' '2016-03-26'\n",
      " '2016-05-01' '2016-05-08' '2016-05-21' '2016-03-27' '2016-05-31'\n",
      " '2016-06-30' '2016-07-01' '2016-07-30' '2016-07-03' '2016-07-31'\n",
      " '2016-07-02' '2016-07-29' '2016-06-11' '2016-09-03' '2016-08-13'\n",
      " '2016-06-26' '2016-08-31' '2016-06-25' '2016-09-30' '2016-10-29'\n",
      " '2016-10-02' '2016-10-31' '2016-10-01' '2016-10-28' '2016-10-30'\n",
      " '2016-08-28' '2016-11-20' '2016-09-25' '2016-11-30' '2016-11-05'\n",
      " '2016-09-24' '2016-12-31' '2017-01-28' '2017-01-31' '2017-01-01'\n",
      " '2017-01-29' '2017-02-12' '2017-02-28' '2017-03-31' '2017-04-29'\n",
      " '2017-04-02' '2017-04-30' '2017-04-01' '2017-02-03' '2017-05-05'\n",
      " '2017-03-25' '2017-05-07' '2017-03-26' '2017-05-31' '2017-05-20'\n",
      " '2017-06-30' '2017-07-29' '2017-07-02' '2017-07-31' '2017-07-01'\n",
      " '2017-08-04' '2017-06-17' '2017-07-30' '2017-06-25' '2017-08-31'\n",
      " '2017-08-12' '2017-09-30' '2017-09-29' '2017-10-28' '2017-10-01'\n",
      " '2017-10-31' '2017-11-03' '2017-09-09' '2017-10-29' '2017-09-03'\n",
      " '2017-11-26' '2017-09-24' '2017-11-30' '2017-11-04' '2017-12-31'\n",
      " '2018-02-03' '2018-01-31' '2017-12-30' '2018-01-28' '2018-01-27'\n",
      " '2018-02-18' '2018-02-28' '2018-03-31' '2018-03-30' '2018-05-05'\n",
      " '2018-04-01' '2018-04-30' '2018-02-02' '2018-05-04' '2018-03-24'\n",
      " '2018-04-29' '2018-04-28' '2018-05-13' '2018-03-25' '2018-05-31'\n",
      " '2018-05-26' '2018-02-24' '2018-06-30' '2018-06-29' '2018-08-04'\n",
      " '2018-07-01' '2018-07-31' '2018-08-03' '2018-06-16' '2018-07-29'\n",
      " '2018-07-28' '2018-06-24' '2018-08-18' '2018-08-31' '2018-09-30'\n",
      " '2018-09-28' '2018-11-03' '2018-10-31' '2018-09-29' '2018-11-02'\n",
      " '2018-09-08' '2018-10-28' '2018-10-27' '2018-09-02' '2018-11-25'\n",
      " '2018-11-10' '2018-11-30' '2018-12-31' '2019-02-02' '2019-01-31'\n",
      " '2018-12-29' '2018-12-30' '2019-02-03' '2019-01-26' '2019-02-17'\n",
      " '2019-01-27' '2019-02-28' '2019-02-01' '2018-12-01' '2019-03-31'\n",
      " '2019-03-29' '2019-05-04' '2019-04-30' '2019-03-30' '2019-05-03'\n",
      " '2019-03-23' '2019-05-05' '2019-04-27' '2019-05-12' '2019-04-28'\n",
      " '2019-05-25' '2019-05-31' '2019-02-23' '2019-06-30' '2019-06-28'\n",
      " '2019-08-03' '2019-07-31' '2019-06-29' '2019-08-02' '2019-06-15'\n",
      " '2019-08-04' '2019-07-27' '2019-07-28' '2019-08-17' '2019-08-31'\n",
      " '2019-09-30' '2019-09-27' '2019-11-02' '2019-09-29' '2019-10-31'\n",
      " '2019-09-28' '2019-11-01' '2019-09-07' '2019-11-03' '2019-10-26'\n",
      " '2019-09-01' '2019-11-24' '2019-11-09' '2019-11-30' '2019-12-31'\n",
      " '2020-02-01' '2020-01-31' '2019-12-28' '2019-12-29' '2020-02-02'\n",
      " '2020-01-25' '2020-02-16' '2020-02-29' '2020-03-31' '2020-03-27'\n",
      " '2020-05-02' '2020-03-29' '2020-04-30' '2020-03-28' '2020-05-01'\n",
      " '2020-03-21' '2020-05-03' '2020-04-25' '2020-05-10' '2020-05-23'\n",
      " '2020-05-31' '2020-06-30' '2020-06-26' '2020-08-01' '2020-06-28'\n",
      " '2020-07-31' '2020-06-27' '2020-06-13' '2020-08-31' '2020-08-02'\n",
      " '2020-07-25' '2020-08-15' '2020-06-20' '2020-09-30' '2020-09-25'\n",
      " '2020-10-31' '2020-09-27' '2020-09-26' '2020-10-30' '2020-09-05'\n",
      " '2020-10-03' '2020-11-01' '2020-10-24' '2020-08-30' '2020-11-22'\n",
      " '2020-11-07' '2020-11-30' '2020-09-12' '2020-12-31' '2020-12-26'\n",
      " '2021-01-02' '2021-01-03' '2021-01-31' '2020-12-05']\n",
      "['TJX' 'MRK' 'KR' 'ADM' 'CAT' 'DOW' 'GE' 'LOW' 'T' 'HON' 'NOC' 'INTC'\n",
      " 'SYY' 'MCK' 'COP' 'CVX' 'MET' 'CI' 'VLO' 'ABC' 'WFC' 'GS' 'ACI' 'JPM'\n",
      " 'JNJ' 'DELL' 'BBY' 'KO' 'AIG' 'C' 'HPQ' 'IBM' 'ALL' 'DE' 'TGT' 'BA' 'LMT'\n",
      " 'AAPL' 'MSFT' 'AMZN' 'PEP' 'HUM' 'VZ' 'PG' 'GD' 'CVS' 'UNH' 'AXP' 'CAH'\n",
      " 'XOM' 'BAC' 'COF' 'UPS' 'MS' 'DIS' 'CSCO' 'HD' 'WMT' 'COST' 'EXC' 'DAL'\n",
      " 'CNC' 'ORCL' 'NKE' 'INT' 'TECD' 'FDX' 'TSN' 'UAL' 'PRU' 'HCA' 'F' 'FMCC'\n",
      " 'BRKA' 'PFE' 'GM' 'MPC' 'CHTR' 'PSX' 'FB' 'ET' 'ABBV' 'PGR' 'PAGP' 'ANTM'\n",
      " 'AAL' 'WBA' 'RTX']\n"
     ]
    }
   ],
   "source": [
    "print(cf_df.head())\n",
    "print(cf_df.info())\n",
    "print(cf_df.describe())\n",
    "print(cf_df.endDate.unique())\n",
    "print(cf_df.symbol.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               label  \\\n",
      "0                                                      \n",
      "1  Cost of sales, including buying and occupancy ...   \n",
      "2                                                      \n",
      "3                              Interest expense, net   \n",
      "4                                                      \n",
      "\n",
      "                                             concept unit        value  \\\n",
      "0                                           Revenues  usd  2.02884e+10   \n",
      "1    tjx:CostOfSalesIncludingBuyingAndOccupancyCosts  usd -1.49684e+10   \n",
      "2             SellingGeneralAndAdministrativeExpense  usd  3.32894e+09   \n",
      "3                             tjx:InterestExpenseNet  usd   3.9509e+07   \n",
      "4  IncomeLossFromContinuingOperationsBeforeIncome...  usd  1.95156e+09   \n",
      "\n",
      "    startDate     endDate  year quarter symbol  \n",
      "0  2009-02-01  2010-01-30  2010      FY    TJX  \n",
      "1  2009-02-01  2010-01-30  2010      FY    TJX  \n",
      "2  2009-02-01  2010-01-30  2010      FY    TJX  \n",
      "3  2009-02-01  2010-01-30  2010      FY    TJX  \n",
      "4  2009-02-01  2010-01-30  2010      FY    TJX  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 111823 entries, 0 to 111822\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   label      111823 non-null  object\n",
      " 1   concept    111823 non-null  object\n",
      " 2   unit       111823 non-null  object\n",
      " 3   value      111823 non-null  object\n",
      " 4   startDate  111823 non-null  object\n",
      " 5   endDate    111823 non-null  object\n",
      " 6   year       111823 non-null  object\n",
      " 7   quarter    111823 non-null  object\n",
      " 8   symbol     111823 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 7.7+ MB\n",
      "None\n",
      "         label                  concept    unit     value   startDate  \\\n",
      "count   111823                   111823  111823  111823.0      111823   \n",
      "unique    8695                     1613       4   34099.0         205   \n",
      "top             IncomeTaxExpenseBenefit     usd       0.0  2017-01-01   \n",
      "freq     18624                     3600   93837    2163.0        8372   \n",
      "\n",
      "           endDate    year quarter  symbol  \n",
      "count       111823  111823  111823  111823  \n",
      "unique         533      13       5      88  \n",
      "top     2016-12-31    2016      Q2     WFC  \n",
      "freq          2414   11457   28092    2713  \n",
      "['2010-01-30' '2009-12-31' '2010-03-31' '2010-03-27' '2010-04-04'\n",
      " '2010-05-01' '2010-04-30' '2010-02-27' '2010-04-02' '2010-05-22'\n",
      " '2010-03-28' '2010-03-20' '2010-04-03' '2010-05-02' '2010-05-09'\n",
      " '2010-06-30' '2010-06-26' '2010-07-04' '2010-07-31' '2010-07-03'\n",
      " '2010-07-30' '2010-07-02' '2010-05-29' '2010-08-14' '2010-05-31'\n",
      " '2010-06-12' '2010-06-27' '2010-08-01' '2010-09-30' '2010-09-25'\n",
      " '2010-10-02' '2010-10-03' '2010-10-30' '2010-10-29' '2010-10-01'\n",
      " '2010-10-31' '2010-08-28' '2010-11-06' '2010-08-31' '2010-08-29'\n",
      " '2010-09-26' '2010-11-21' '2010-12-31' '2011-01-28' '2011-01-01'\n",
      " '2010-12-25' '2011-01-02' '2011-02-28' '2011-01-29' '2011-01-31'\n",
      " '2010-11-27' '2010-11-30' '2011-02-13' '2011-01-30' '2011-03-31'\n",
      " '2011-04-29' '2011-04-02' '2011-04-03' '2011-04-30' '2011-02-26'\n",
      " '2011-04-01' '2011-05-21' '2011-03-26' '2011-03-27' '2011-03-19'\n",
      " '2011-05-01' '2011-05-08' '2011-05-31' '2011-06-30' '2011-08-27'\n",
      " '2011-07-29' '2011-07-02' '2011-07-03' '2011-07-30' '2011-08-31'\n",
      " '2011-07-01' '2011-07-31' '2011-05-28' '2011-08-13' '2011-06-25'\n",
      " '2011-06-11' '2011-06-26' '2011-09-30' '2011-10-30' '2011-10-28'\n",
      " '2011-10-01' '2011-10-02' '2011-10-29' '2011-11-30' '2011-10-31'\n",
      " '2011-11-05' '2011-09-03' '2011-08-28' '2011-09-24' '2011-09-25'\n",
      " '2011-11-20' '2011-12-31' '2012-01-29' '2011-11-26' '2012-02-03'\n",
      " '2012-01-31' '2012-01-28' '2012-01-01' '2012-02-12' '2012-02-29'\n",
      " '2012-03-31' '2012-03-30' '2012-05-04' '2012-04-01' '2012-04-29'\n",
      " '2012-03-03' '2012-05-05' '2012-04-30' '2012-04-28' '2012-05-19'\n",
      " '2012-03-24' '2012-03-25' '2012-05-06' '2012-05-31' '2012-06-30'\n",
      " '2012-06-29' '2012-08-03' '2012-07-31' '2012-07-01' '2012-07-29'\n",
      " '2012-08-04' '2012-07-28' '2012-08-11' '2012-06-24' '2012-06-16'\n",
      " '2012-08-31' '2012-09-30' '2012-09-28' '2012-11-02' '2012-09-29'\n",
      " '2012-10-31' '2012-10-28' '2012-11-03' '2012-10-27' '2012-09-02'\n",
      " '2012-11-30' '2012-11-25' '2012-09-08' '2012-12-31' '2012-12-29'\n",
      " '2013-01-31' '2012-12-30' '2013-02-03' '2013-02-01' '2013-01-26'\n",
      " '2013-02-02' '2013-02-17' '2013-02-28' '2013-03-31' '2013-03-29'\n",
      " '2013-03-30' '2013-05-03' '2013-03-23' '2013-04-30' '2013-05-05'\n",
      " '2013-05-04' '2013-04-27' '2013-05-25' '2013-05-31' '2013-05-12'\n",
      " '2013-06-30' '2013-06-28' '2013-06-29' '2013-08-02' '2013-06-15'\n",
      " '2013-07-31' '2013-08-04' '2013-08-03' '2013-07-27' '2013-08-17'\n",
      " '2013-08-31' '2013-09-30' '2013-09-27' '2013-11-02' '2013-09-29'\n",
      " '2013-09-28' '2013-11-01' '2013-09-07' '2013-10-31' '2013-11-03'\n",
      " '2013-10-26' '2013-11-24' '2013-11-09' '2013-11-30' '2013-09-01'\n",
      " '2013-12-31' '2014-02-01' '2013-12-28' '2014-01-31' '2013-12-29'\n",
      " '2014-02-02' '2014-01-25' '2014-02-16' '2014-02-28' '2014-03-31'\n",
      " '2014-03-28' '2014-05-03' '2014-03-30' '2014-03-29' '2014-05-02'\n",
      " '2014-03-22' '2014-04-30' '2014-05-04' '2014-04-26' '2014-05-11'\n",
      " '2014-05-31' '2014-06-30' '2014-06-27' '2014-08-02' '2014-06-29'\n",
      " '2014-06-28' '2014-08-01' '2014-06-14' '2014-07-31' '2014-08-03'\n",
      " '2014-07-26' '2014-05-24' '2014-08-16' '2014-08-31' '2014-09-30'\n",
      " '2014-09-26' '2014-11-01' '2014-09-28' '2014-09-27' '2014-10-31'\n",
      " '2014-09-06' '2014-11-02' '2014-10-25' '2014-11-23' '2014-11-08'\n",
      " '2014-11-30' '2014-12-31' '2015-01-31' '2014-12-27' '2015-01-30'\n",
      " '2014-12-28' '2015-02-01' '2015-01-24' '2015-02-15' '2015-02-28'\n",
      " '2015-03-31' '2015-04-03' '2015-05-02' '2015-04-05' '2015-03-28'\n",
      " '2015-05-01' '2015-03-21' '2015-03-29' '2015-04-30' '2015-05-03'\n",
      " '2015-04-25' '2015-05-10' '2015-05-23' '2015-05-31' '2015-06-30'\n",
      " '2015-07-03' '2015-08-01' '2015-07-05' '2015-06-27' '2015-07-31'\n",
      " '2015-06-13' '2015-06-28' '2015-08-02' '2015-07-25' '2015-08-15'\n",
      " '2015-08-31' '2015-09-30' '2015-10-02' '2015-10-31' '2015-10-04'\n",
      " '2015-09-26' '2015-10-30' '2015-09-05' '2015-09-27' '2015-10-03'\n",
      " '2015-11-01' '2015-10-24' '2015-08-30' '2015-11-22' '2015-11-07'\n",
      " '2015-11-30' '2015-12-31' '2016-01-30' '2015-12-26' '2016-01-29'\n",
      " '2016-01-02' '2016-01-31' '2016-01-03' '2016-01-23' '2016-02-14'\n",
      " '2016-02-29' '2016-03-31' '2016-04-01' '2016-04-30' '2016-04-03'\n",
      " '2016-04-02' '2016-04-29' '2016-03-19' '2016-03-26' '2016-05-01'\n",
      " '2016-05-08' '2016-05-21' '2016-03-27' '2016-05-31' '2016-06-30'\n",
      " '2016-07-01' '2016-07-30' '2016-07-03' '2016-07-31' '2016-07-02'\n",
      " '2016-07-29' '2016-06-11' '2016-09-03' '2016-08-13' '2016-06-26'\n",
      " '2016-08-31' '2016-06-25' '2016-09-30' '2016-10-29' '2016-10-02'\n",
      " '2016-10-31' '2016-10-01' '2016-10-28' '2016-10-30' '2016-08-28'\n",
      " '2016-11-20' '2016-09-25' '2016-11-30' '2016-11-05' '2016-09-24'\n",
      " '2016-12-31' '2017-01-28' '2017-01-31' '2017-01-01' '2017-01-29'\n",
      " '2017-02-12' '2017-02-28' '2017-03-31' '2017-04-29' '2017-04-02'\n",
      " '2017-04-30' '2017-04-01' '2017-02-03' '2017-05-05' '2017-03-25'\n",
      " '2017-05-07' '2017-03-26' '2017-05-31' '2017-05-20' '2017-06-30'\n",
      " '2017-07-29' '2017-07-02' '2017-07-31' '2017-07-01' '2017-08-04'\n",
      " '2017-06-17' '2017-07-30' '2017-06-25' '2017-08-31' '2017-08-12'\n",
      " '2017-09-30' '2017-09-29' '2017-10-28' '2017-10-01' '2017-10-31'\n",
      " '2017-11-03' '2017-09-09' '2017-10-29' '2017-09-03' '2017-11-26'\n",
      " '2017-09-24' '2017-11-30' '2017-11-04' '2017-12-31' '2018-02-03'\n",
      " '2018-01-31' '2017-12-30' '2018-01-28' '2018-01-27' '2018-02-18'\n",
      " '2018-02-28' '2018-03-31' '2018-03-30' '2018-05-05' '2018-04-01'\n",
      " '2018-04-30' '2018-02-02' '2018-05-04' '2018-03-24' '2018-04-29'\n",
      " '2018-04-28' '2018-05-13' '2018-03-25' '2018-05-31' '2018-05-26'\n",
      " '2018-02-24' '2018-06-30' '2018-06-29' '2018-08-04' '2018-07-01'\n",
      " '2018-07-31' '2018-08-03' '2018-06-16' '2018-07-29' '2018-07-28'\n",
      " '2018-06-24' '2018-08-18' '2018-08-31' '2018-09-30' '2018-09-28'\n",
      " '2018-11-03' '2018-10-31' '2018-09-29' '2018-11-02' '2018-09-08'\n",
      " '2018-10-28' '2018-10-27' '2018-09-02' '2018-11-25' '2018-11-10'\n",
      " '2018-11-30' '2018-12-31' '2019-02-02' '2019-01-31' '2018-12-29'\n",
      " '2018-12-30' '2019-02-03' '2019-01-26' '2019-02-17' '2019-01-27'\n",
      " '2019-02-28' '2019-02-01' '2018-12-01' '2019-03-31' '2019-03-29'\n",
      " '2019-05-04' '2019-04-30' '2019-03-30' '2019-05-03' '2019-03-23'\n",
      " '2019-05-05' '2019-04-27' '2019-05-12' '2019-04-28' '2019-05-25'\n",
      " '2019-05-31' '2019-02-23' '2019-06-30' '2019-06-28' '2019-08-03'\n",
      " '2019-07-31' '2019-06-29' '2019-08-02' '2019-06-15' '2019-08-04'\n",
      " '2019-07-27' '2019-07-28' '2019-08-17' '2019-08-31' '2019-09-30'\n",
      " '2019-09-27' '2019-11-02' '2019-09-29' '2019-10-31' '2019-09-28'\n",
      " '2019-11-01' '2019-09-07' '2019-11-03' '2019-10-26' '2019-09-01'\n",
      " '2019-11-24' '2019-11-09' '2019-11-30' '2019-12-31' '2020-02-01'\n",
      " '2020-01-31' '2019-12-28' '2019-12-29' '2020-02-02' '2020-01-25'\n",
      " '2020-02-16' '2020-02-29' '2020-03-31' '2020-03-27' '2020-05-02'\n",
      " '2020-03-29' '2020-04-30' '2020-03-28' '2020-05-01' '2020-03-21'\n",
      " '2020-05-03' '2020-04-25' '2020-05-10' '2020-05-23' '2020-05-31'\n",
      " '2020-06-30' '2020-06-26' '2020-08-01' '2020-06-28' '2020-07-31'\n",
      " '2020-06-27' '2020-06-13' '2020-08-31' '2020-08-02' '2020-07-25'\n",
      " '2020-08-15' '2020-06-20' '2020-09-30' '2020-09-25' '2020-10-31'\n",
      " '2020-09-27' '2020-09-26' '2020-10-30' '2020-09-05' '2020-10-03'\n",
      " '2020-11-01' '2020-10-24' '2020-08-30' '2020-11-22' '2020-11-07'\n",
      " '2020-11-30' '2020-09-12' '2020-12-31' '2020-12-26' '2021-01-02'\n",
      " '2021-01-03' '2021-01-31' '2020-12-05']\n",
      "['TJX' 'MRK' 'KR' 'ADM' 'CAT' 'DOW' 'GE' 'T' 'HON' 'NOC' 'INTC' 'SYY'\n",
      " 'MCK' 'COP' 'CVX' 'MET' 'CI' 'VLO' 'ABC' 'WFC' 'GS' 'ACI' 'JPM' 'JNJ'\n",
      " 'DELL' 'BBY' 'KO' 'AIG' 'C' 'HPQ' 'IBM' 'ALL' 'DE' 'TGT' 'BA' 'LMT'\n",
      " 'AAPL' 'MSFT' 'AMZN' 'PEP' 'HUM' 'VZ' 'PG' 'GD' 'CVS' 'UNH' 'AXP' 'CAH'\n",
      " 'XOM' 'BAC' 'COF' 'UPS' 'MS' 'PRU' 'PGR' 'DIS' 'CSCO' 'HD' 'WMT' 'COST'\n",
      " 'EXC' 'DAL' 'CNC' 'ORCL' 'NKE' 'INT' 'TSN' 'TECD' 'LOW' 'FDX' 'UAL' 'HCA'\n",
      " 'F' 'FMCC' 'BRKA' 'PFE' 'GM' 'MPC' 'CHTR' 'PSX' 'FB' 'ET' 'ABBV' 'PAGP'\n",
      " 'ANTM' 'AAL' 'WBA' 'RTX']\n"
     ]
    }
   ],
   "source": [
    "print(ic_df.head())\n",
    "print(ic_df.info())\n",
    "print(ic_df.describe())\n",
    "print(ic_df.endDate.unique())\n",
    "print(ic_df.symbol.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some things I needed to find out:\n",
    "- Why are all these reports split into records called \"bs/cf/ic\"? What are those abbreviations for?\n",
    "- how about... **B**alance **S**heet, **C**ash **F**low, **I**n**C**ome statement? https://www.sec.gov/oiea/reportspubs/investor-publications/beginners-guide-to-financial-statements.html\n",
    "- I want to see more of what's in those \"data\" fields: learning how to unpack nested JSON\n",
    "- Describe doesn't do much when every number is a different thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Already I'm seeing:\n",
    "- Not all the tickers are in all 3 dataframes unless they're showing up in different order ('LOW' is one example)\n",
    "- This might be all the tickers I'm looking for; I count 88 in the income dataframe. Might be not quite. Most likely enough\n",
    "- There's still one thing that's hard to see here, which is how many of the label and/or concept columns are the same across companies and filings. How sparse will this data be once I figure out the categorization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TODO:*** code to save the files to a location I'm willing to upload, code to save the dataframes for later, melt these dataframes and see where the commonalities are (group/sort by label and/or concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'RawData')\n",
    "bs_df.to_csv(os.path.join(save_dir, 'FH-balsheets-tickers.csv'))\n",
    "cf_df.to_csv(os.path.join(save_dir, 'FH-cashflow-tickers.csv'))\n",
    "ic_df.to_csv(os.path.join(save_dir, 'FH-income-tickers.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. The miguelaenlle SEC dataset \"Historical Financials\"\n",
    "- Should be far less complicated! Everything in one CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2743 entries, 11 to 100690\n",
      "Data columns (total 44 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   commonstocksharesissued           2471 non-null   float64\n",
      " 1   assetscurrent                     2375 non-null   float64\n",
      " 2   accountspayablecurrent            2083 non-null   float64\n",
      " 3   commonstockvalue                  2160 non-null   float64\n",
      " 4   liabilities                       1751 non-null   float64\n",
      " 5   liabilitiesandstockholdersequity  2743 non-null   float64\n",
      " 6   stockholdersequity                2503 non-null   float64\n",
      " 7   earningspersharebasic             2739 non-null   float64\n",
      " 8   netincomeloss                     2639 non-null   float64\n",
      " 9   profitloss                        1906 non-null   float64\n",
      " 10  costofgoodssold                   1168 non-null   float64\n",
      " 11  filing_date                       2743 non-null   object \n",
      " 12  costsandexpenses                  1089 non-null   float64\n",
      " 13  cash                              635 non-null    float64\n",
      " 14  notespayable                      345 non-null    float64\n",
      " 15  preferredstockvalue               323 non-null    float64\n",
      " 16  depreciation                      1815 non-null   float64\n",
      " 17  operatingexpenses                 855 non-null    float64\n",
      " 18  revenues                          2072 non-null   float64\n",
      " 19  land                              757 non-null    float64\n",
      " 20  accountsreceivablenet             222 non-null    float64\n",
      " 21  deferredrevenue                   461 non-null    float64\n",
      " 22  grossprofit                       1259 non-null   float64\n",
      " 23  sharesissued                      882 non-null    float64\n",
      " 24  accruedincometaxes                126 non-null    float64\n",
      " 25  sharesoutstanding                 400 non-null    float64\n",
      " 26  borrowedfunds                     18 non-null     float64\n",
      " 27  inventorygross                    196 non-null    float64\n",
      " 28  commercialpaper                   1093 non-null   float64\n",
      " 29  dividends                         421 non-null    float64\n",
      " 30  commonstocknoparvalue             6 non-null      float64\n",
      " 31  costofservices                    500 non-null    float64\n",
      " 32  debtcurrent                       1230 non-null   float64\n",
      " 33  accruedinsurancecurrent           183 non-null    float64\n",
      " 34  officerscompensation              16 non-null     float64\n",
      " 35  intangibleassetscurrent           0 non-null      float64\n",
      " 36  salariesandwages                  39 non-null     float64\n",
      " 37  interestanddebtexpense            385 non-null    float64\n",
      " 38  convertibledebt                   124 non-null    float64\n",
      " 39  assetmanagementcosts              0 non-null      float64\n",
      " 40  accountsreceivablegross           0 non-null      float64\n",
      " 41  directoperatingcosts              0 non-null      float64\n",
      " 42  operatingcycle                    34 non-null     object \n",
      " 43  stock                             2743 non-null   object \n",
      "dtypes: float64(41), object(3)\n",
      "memory usage: 964.3+ KB\n",
      "None\n",
      "       commonstocksharesissued  assetscurrent  accountspayablecurrent  \\\n",
      "count             2.471000e+03   2.375000e+03            2.083000e+03   \n",
      "mean              1.688484e+09   2.560551e+10            7.042524e+09   \n",
      "std               2.282102e+09   2.626522e+10            8.868639e+09   \n",
      "min              -5.950000e+08  -1.504700e+10           -7.028400e+10   \n",
      "25%               3.563371e+08   8.065106e+09            1.897200e+09   \n",
      "50%               7.977000e+08   1.777000e+10            4.828000e+09   \n",
      "75%               1.785419e+09   3.582050e+10            8.777000e+09   \n",
      "max               1.169384e+10   1.755520e+11            6.298500e+10   \n",
      "\n",
      "       commonstockvalue   liabilities  liabilitiesandstockholdersequity  \\\n",
      "count      2.160000e+03  1.751000e+03                      2.743000e+03   \n",
      "mean       1.253432e+09  7.081519e+10                      9.231043e+10   \n",
      "std        2.633283e+09  1.284790e+11                      1.717543e+11   \n",
      "min       -2.417000e+09 -3.806840e+11                     -1.424806e+12   \n",
      "25%        7.000000e+06  8.018182e+09                      2.453150e+10   \n",
      "50%        4.400000e+07  2.961900e+10                      4.649000e+10   \n",
      "75%        9.590000e+08  8.892700e+10                      1.194080e+11   \n",
      "max        2.055900e+10  8.744380e+11                      2.338833e+12   \n",
      "\n",
      "       stockholdersequity  earningspersharebasic  netincomeloss    profitloss  \\\n",
      "count        2.503000e+03            2739.000000   2.639000e+03  1.906000e+03   \n",
      "mean         1.714488e+10               2.311395   2.120869e+09  1.750295e+09   \n",
      "std          6.570956e+10               3.143775   5.569271e+09  9.196982e+09   \n",
      "min         -1.070090e+12             -24.920000  -1.342590e+11 -2.524880e+11   \n",
      "25%          7.360105e+08               0.710000   2.207310e+08  8.500000e+07   \n",
      "50%          9.698000e+09               1.430000   9.910000e+08  9.070000e+08   \n",
      "75%          3.161150e+10               2.900000   2.977000e+09  3.208000e+09   \n",
      "max          1.987410e+11              44.220000   5.953100e+10  4.522000e+10   \n",
      "\n",
      "       ...   debtcurrent  accruedinsurancecurrent  officerscompensation  \\\n",
      "count  ...  1.230000e+03             1.830000e+02                  16.0   \n",
      "mean   ...  4.782791e+09             3.486634e+09             4800000.0   \n",
      "std    ...  7.917039e+09             5.386842e+09                   0.0   \n",
      "min    ... -9.100000e+07             1.300000e+07             4800000.0   \n",
      "25%    ...  2.470000e+08             6.598300e+07             4800000.0   \n",
      "50%    ...  1.984000e+09             6.750000e+08             4800000.0   \n",
      "75%    ...  5.940000e+09             3.447000e+09             4800000.0   \n",
      "max    ...  5.117900e+10             1.410000e+10             4800000.0   \n",
      "\n",
      "       intangibleassetscurrent  salariesandwages  interestanddebtexpense  \\\n",
      "count                      0.0      3.900000e+01            3.850000e+02   \n",
      "mean                       NaN      1.431136e+08            2.065504e+08   \n",
      "std                        NaN      1.932954e+08            3.864973e+08   \n",
      "min                        NaN      2.450000e+05           -1.735000e+09   \n",
      "25%                        NaN      2.450000e+05            6.100000e+07   \n",
      "50%                        NaN      2.800000e+07            1.220000e+08   \n",
      "75%                        NaN      2.380000e+08            2.870000e+08   \n",
      "max                        NaN      6.070000e+08            1.600000e+09   \n",
      "\n",
      "       convertibledebt  assetmanagementcosts  accountsreceivablegross  \\\n",
      "count     1.240000e+02                   0.0                      0.0   \n",
      "mean      6.128724e+08                   NaN                      NaN   \n",
      "std       8.159051e+08                   NaN                      NaN   \n",
      "min       5.000000e+06                   NaN                      NaN   \n",
      "25%       9.000000e+06                   NaN                      NaN   \n",
      "50%       3.440000e+08                   NaN                      NaN   \n",
      "75%       7.360000e+08                   NaN                      NaN   \n",
      "max       3.000000e+09                   NaN                      NaN   \n",
      "\n",
      "       directoperatingcosts  \n",
      "count                   0.0  \n",
      "mean                    NaN  \n",
      "std                     NaN  \n",
      "min                     NaN  \n",
      "25%                     NaN  \n",
      "50%                     NaN  \n",
      "75%                     NaN  \n",
      "max                     NaN  \n",
      "\n",
      "[8 rows x 41 columns]\n",
      "['AAL' 'AAPL' 'ABBV' 'ABC' 'ACI' 'ADM' 'AIG' 'ALL' 'AMZN' 'ANTM' 'AXP'\n",
      " 'BAC' 'BA' 'BBY' 'CAH' 'CAT' 'CHTR' 'CI' 'CNC' 'COF' 'COP' 'COST' 'CSCO'\n",
      " 'CVS' 'CVX' 'DAL' 'DE' 'DOW' 'EXC' 'FB' 'FDX' 'F' 'GD' 'GE' 'GM' 'GOOGL'\n",
      " 'HCA' 'HD' 'HON' 'HPQ' 'HUM' 'IBM' 'INTC' 'INT' 'JNJ' 'KO' 'KR' 'LMT'\n",
      " 'LOW' 'MCK' 'MET' 'MPC' 'MRK' 'MSFT' 'MS' 'NKE' 'NOC' 'ORCL' 'PAGP' 'PEP'\n",
      " 'PFE' 'PGR' 'PG' 'PRU' 'PSX' 'SYY' 'TECD' 'TGT' 'TJX' 'TSN' 'T' 'UAL'\n",
      " 'UNH' 'UPS' 'VLO' 'VZ' 'WBA' 'WFC' 'WMT' 'XOM']\n"
     ]
    }
   ],
   "source": [
    "hf_path = os.path.join(os.getcwd(), 'SEC-HistoricalFinancials', 'quarterly_financials.csv')\n",
    "hf_df = pd.read_csv(hf_path, index_col=0, low_memory=False)\n",
    "# dtype warning for mixed types in column 43? maybe this is about the first ticker being 'AA$'?\n",
    "# set low_memory as the warning suggested because I'm only using this DF to look for the tickers I want\n",
    "hf_tickers_df = hf_df[hf_df['stock'].isin(ticker_list)]\n",
    "print(hf_tickers_df.info())\n",
    "print(hf_tickers_df.describe())\n",
    "print(hf_tickers_df.stock.unique())\n",
    "\n",
    "# This dataframe with one ticker held 39 records and used ~14K memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tickers_df.to_csv(os.path.join(save_dir, 'HF-financials-tickers.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see a few entirely blank columns here, get rid of those later.  \n",
    "How do I want to line up this information with the Finnhub data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. The borismarjanovic 'Huge Dataset' of market prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: 82\n",
      "Problem files found: 10\n",
      "         Date     Open     High      Low    Close   Volume  OpenInt ticker\n",
      "0  1972-03-20  0.03403  0.03403  0.03403  0.03403  3132119        0    wmt\n",
      "1  1972-03-22  0.03403  0.04251  0.03403  0.03403  1084194        0    wmt\n",
      "2  1972-03-23  0.03403  0.04251  0.03403  0.03403   783030        0    wmt\n",
      "3  1972-03-24  0.03403  0.04251  0.03403  0.03403  1264892        0    wmt\n",
      "4  1972-03-27  0.04251  0.04251  0.04251  0.04251  1385361        0    wmt\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 629798 entries, 0 to 629797\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   Date     629798 non-null  object \n",
      " 1   Open     629798 non-null  float64\n",
      " 2   High     629798 non-null  float64\n",
      " 3   Low      629798 non-null  float64\n",
      " 4   Close    629798 non-null  float64\n",
      " 5   Volume   629798 non-null  int64  \n",
      " 6   OpenInt  629798 non-null  int64  \n",
      " 7   ticker   629798 non-null  object \n",
      "dtypes: float64(4), int64(2), object(2)\n",
      "memory usage: 38.4+ MB\n",
      "None\n",
      "                Open           High            Low          Close  \\\n",
      "count  629798.000000  629798.000000  629798.000000  629798.000000   \n",
      "mean       83.177361      84.191991      82.131698      83.151576   \n",
      "std       507.211970     514.125328     500.225545     506.937885   \n",
      "min         0.007910       0.007910       0.007910       0.007910   \n",
      "25%         4.835900       4.889700       4.775700       4.836700   \n",
      "50%        19.060000      19.315000      18.786000      19.060000   \n",
      "75%        42.183000      42.677000      41.707000      42.197000   \n",
      "max     11720.240000   11768.390000   11400.200000   11697.130000   \n",
      "\n",
      "             Volume   OpenInt  \n",
      "count  6.297980e+05  629798.0  \n",
      "mean   1.235980e+07       0.0  \n",
      "std    2.951243e+07       0.0  \n",
      "min    0.000000e+00       0.0  \n",
      "25%    1.542592e+06       0.0  \n",
      "50%    4.278342e+06       0.0  \n",
      "75%    1.037416e+07       0.0  \n",
      "max    2.423735e+09       0.0  \n"
     ]
    }
   ],
   "source": [
    "hd_path = os.path.join(os.getcwd(), 'Market-HugeDataset', 'Stocks')\n",
    "hd_filelist = [os.path.join(hd_path, t.lower() + '.us.txt') for t in ticker_list]\n",
    "# print(hd_filelist[:6])    \n",
    "hd_df_list = []\n",
    "hd_problem_files = []\n",
    "for file in hd_filelist:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "    except FileNotFoundError as fnf_err:\n",
    "        hd_problem_files.append(file)\n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        raise\n",
    "    else:\n",
    "        df['ticker'] = str(os.path.split(file)[-1]).split('.')[0]\n",
    "        #print(df.head())\n",
    "        hd_df_list.append(df)\n",
    "# concat/map solution found at https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe\n",
    "#pd.concat(map(lambda f: pd.read_csv(f, keys=os.path.split(f)[-1]), hd_filelist)) nope, there's no keys arg to read_csv\n",
    "# oops, I'm going to need some way to mark which ticker these go with!!\n",
    "\n",
    "print('Files found: ' + str(len(hd_df_list)))\n",
    "print('Problem files found: ' + str(len(hd_problem_files)))\n",
    "#print(hd_problem_files)\n",
    "# Okay, so one or two of these problem files is a \"maybe\" but I'm not sure enough, skip them and go with the matches.\n",
    "hd_df = pd.concat(hd_df_list, ignore_index=True)\n",
    "print(hd_df.head())\n",
    "print(hd_df.info())\n",
    "print(hd_df.describe())\n",
    "\n",
    "# This is likely to still be somewhat big. This dataframe with one ticker held 8364 records and used 458K memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Notes from original run)\n",
    "\n",
    "Do I believe these AAPL prices? OK, a quick search says today's price was `$`134.16 at last close. Wonder if I'll see what the effect of adjustment is by comparing with the next dataset? I'm pretty sure I've noticed AAPL's price being higher than `$`175 in my random contacts with stock market reports.\n",
    "\n",
    "(Notes from expanded tickers)\n",
    "\n",
    "Wow, Walmart goes back to 1972 and prices were 3 cents a share? I really don't know if I believe these prices. At minimum I want to know a lot more about how he \"adjusted\" them. Was inflation accounted for? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_df.to_csv(os.path.join(save_dir, 'HD-market-tickers.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. The tsaustin 'Historical Prices' market dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    symbol  total_prices stock_from_date stock_to_date  total_earnings  \\\n",
      "9      AAL          3811      2005-09-27    2020-11-13              28   \n",
      "16    AAPL          5756      1998-01-02    2020-11-13              46   \n",
      "27    ABBV          1975      2013-01-02    2020-11-03              31   \n",
      "28     ABC          5748      1998-01-02    2020-11-03              46   \n",
      "107    ADM          5750      1998-01-02    2020-11-05              47   \n",
      "\n",
      "    earnings_from_date earnings_to_date  \n",
      "9           2014-01-28       2020-10-22  \n",
      "16          2009-07-21       2020-10-29  \n",
      "27          2013-04-26       2020-10-30  \n",
      "28          2009-07-30       2020-11-05  \n",
      "107         2009-05-05       2020-10-29  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 86 entries, 9 to 7675\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   symbol              86 non-null     object\n",
      " 1   total_prices        86 non-null     int64 \n",
      " 2   stock_from_date     86 non-null     object\n",
      " 3   stock_to_date       86 non-null     object\n",
      " 4   total_earnings      86 non-null     int64 \n",
      " 5   earnings_from_date  85 non-null     object\n",
      " 6   earnings_to_date    85 non-null     object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 5.4+ KB\n",
      "None\n",
      "    symbol        date      qtr  eps_est   eps release_time\n",
      "99     AAL  2014-01-28  12/2013     0.54  1.51          pre\n",
      "100    AAL  2014-04-24  03/2014     0.50  0.54          pre\n",
      "101    AAL  2014-07-24  06/2014     1.94  1.98          pre\n",
      "102    AAL  2014-10-23  09/2014     1.63  1.66          pre\n",
      "103    AAL  2015-01-27  12/2014     1.51  1.52          pre\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3647 entries, 99 to 158954\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   symbol        3647 non-null   object \n",
      " 1   date          3647 non-null   object \n",
      " 2   qtr           3637 non-null   object \n",
      " 3   eps_est       2677 non-null   float64\n",
      " 4   eps           2827 non-null   float64\n",
      " 5   release_time  3086 non-null   object \n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 199.4+ KB\n",
      "None\n",
      "  symbol        date  dividend\n",
      "0   MSFT  2016-11-15      0.39\n",
      "1   MSFT  2011-05-17      0.16\n",
      "2   MSFT  2008-05-13      0.11\n",
      "3   MSFT  2011-02-15      0.16\n",
      "4   MSFT  2012-02-14      0.20\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5658 entries, 0 to 250078\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   symbol    5658 non-null   object \n",
      " 1   date      5658 non-null   object \n",
      " 2   dividend  5658 non-null   float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 176.8+ KB\n",
      "None\n",
      "  symbol        date   open   high    low  close  close_adjusted     volume  \\\n",
      "0   MSFT  2016-05-16  50.80  51.96  50.75  51.83         49.7013   20032017   \n",
      "1   MSFT  2002-01-16  68.85  69.84  67.85  67.87         22.5902   30977700   \n",
      "2   MSFT  2001-09-18  53.41  55.00  53.17  54.32         18.0802   41591300   \n",
      "3   MSFT  2007-10-26  36.01  36.03  34.56  35.03         27.2232  288121200   \n",
      "4   MSFT  2014-06-27  41.61  42.29  41.51  42.25         38.6773   74640000   \n",
      "\n",
      "   split_coefficient  \n",
      "0                1.0  \n",
      "1                1.0  \n",
      "2                1.0  \n",
      "3                1.0  \n",
      "4                1.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 437565 entries, 0 to 23526281\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   symbol             437565 non-null  object \n",
      " 1   date               437565 non-null  object \n",
      " 2   open               437565 non-null  float64\n",
      " 3   high               437565 non-null  float64\n",
      " 4   low                437565 non-null  float64\n",
      " 5   close              437565 non-null  float64\n",
      " 6   close_adjusted     437565 non-null  float64\n",
      " 7   volume             437565 non-null  int64  \n",
      " 8   split_coefficient  437565 non-null  float64\n",
      "dtypes: float64(6), int64(1), object(2)\n",
      "memory usage: 33.4+ MB\n",
      "None\n",
      "                open           high            low          close  \\\n",
      "count  437565.000000  437565.000000  437565.000000  437565.000000   \n",
      "mean       75.028236      75.860043      74.168409      75.033509   \n",
      "std       119.470067     120.683404     118.121240     119.442594   \n",
      "min         0.000000       0.000000       0.000000       0.350000   \n",
      "25%        33.730000      34.170000      33.250000      33.740000   \n",
      "50%        52.160000      52.790000      51.540000      52.170000   \n",
      "75%        80.300000      81.150000      79.500000      80.310000   \n",
      "max      3547.000000    3552.250000    3486.685000    3531.450000   \n",
      "\n",
      "       close_adjusted        volume  split_coefficient  \n",
      "count   437565.000000  4.375650e+05      437565.000000  \n",
      "mean        66.732487  1.146615e+07           1.000275  \n",
      "std        136.482115  3.372398e+07           0.020798  \n",
      "min          0.232700  0.000000e+00           0.050000  \n",
      "25%         20.544800  2.067300e+06           1.000000  \n",
      "50%         35.949900  4.509700e+06           1.000000  \n",
      "75%         64.990100  1.044510e+07           1.000000  \n",
      "max       3531.450000  3.948220e+09           7.000000  \n"
     ]
    }
   ],
   "source": [
    "hp_filepath = os.path.join(os.getcwd(), 'Market-HistoricalPrices')\n",
    "hp_df_s = pd.read_csv(os.path.join(hp_filepath, 'dataset_summary.csv'))\n",
    "hp_df_tckr_s = hp_df_s[hp_df_s['symbol'].isin(ticker_list)]\n",
    "print(hp_df_tckr_s.head())\n",
    "print(hp_df_tckr_s.info())\n",
    "hp_df_e = pd.read_csv(os.path.join(hp_filepath, 'stocks_latest', 'earnings_latest.csv'))\n",
    "hp_df_tckr_e = hp_df_e[hp_df_e['symbol'].isin(ticker_list)]\n",
    "print(hp_df_tckr_e.head())\n",
    "print(hp_df_tckr_e.info())\n",
    "hp_df_d = pd.read_csv(os.path.join(hp_filepath, 'stocks_latest', 'dividends_latest.csv'))\n",
    "hp_df_tckr_d = hp_df_d[hp_df_d['symbol'].isin(ticker_list)]\n",
    "print(hp_df_tckr_d.head())\n",
    "print(hp_df_tckr_d.info())\n",
    "hp_df_p = pd.read_csv(os.path.join(hp_filepath, 'stocks_latest', 'stock_prices_latest.csv'))\n",
    "hp_df_tckr_p = hp_df_p[hp_df_p['symbol'].isin(ticker_list)]\n",
    "print(hp_df_tckr_p.head())\n",
    "print(hp_df_tckr_p.info())\n",
    "print(hp_df_tckr_p.describe())\n",
    "# First run with single ticker, 1 entry in summary, 46 entries in earnings, 34 entries in dividends, ~3K memory for all 3 DFs\n",
    "# Also first run, 5756 entries and 450K memory for price DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Notes from first run)\n",
    "\n",
    "See? This one suggests much higher max prices. What's going on here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df_tckr_s.to_csv(os.path.join(save_dir, 'HP-market-ticker-summary.csv'))\n",
    "hp_df_tckr_e.to_csv(os.path.join(save_dir, 'HP-market-ticker-earnings.csv'))\n",
    "hp_df_tckr_d.to_csv(os.path.join(save_dir, 'HP-market-ticker-dividends.csv'))\n",
    "hp_df_tckr_p.to_csv(os.path.join(save_dir, 'HP-market-ticker-prices.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choose 100 companies and pull their data into new files to reduce data sizes. \n",
    "Let's try the 2020 Fortune 100.\n",
    "Data retrieved from https://fortune.com/fortune500/2020/search/?rank=asc\n",
    "Spent a bunch of time massaging the text as copied back into tabular format. Spent a bunch more time going through each company's profile page to pull out ticker symbols. Hopefully an employer would have a better way to access the data, but it's good enough for this project. They want \\$500 or more for a data file and don't talk about having an API that I can see.\n",
    "\n",
    "- Some ticker notes:\n",
    "    - There are a bunch of companies (especially mutual insurance companies) without tickers, so I'm going to end up with less than 100 companies. Expand search or leave it?\n",
    "    - Will I need to look out for name changes (esp ticker changes)?\n",
    "    - Albertsons has a ticker but IPO was last year sometime so may not have data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was done by adjusting code so it can all iterate over a list of tickers and pull out selected data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Begin EDA, look for cleaning needs. \n",
    "- Figure out how best to pull only the SEC-related dates of market data, assuming that dates will generally be different for each company.\n",
    "- Calculate (can I sanity-check somehow?) fundamental analysis ratios: P/E, P/E/G, price/book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am so starting a new damn notebook for the next wrangling steps and EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other tasks as I think of them while exploring!\n",
    "- Do I have data that shows market capitalization? Or shares outstanding, so I can calculate?\n",
    "- How does one adjust prices for dividends, splits? What price do I need to use to calculate ratios, market cap, etc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for later:\n",
    "- Once I produce something with this, should I share it on Kaggle? If I've combined datasets, can I link my code to both of them?\n",
    "- Can or should I try to confirm that Yahoo Finance is a bad place to get stock data from? (see notes on first market dataset above)\n",
    "- How do I pull out the data I want from these JSON files and save for later? As dataframe, csv, something else?\n",
    "- Can I pull additional company data from the Fortune site? Should I? https://fortune.com/company/stonex-group/fortune500/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research:\n",
    "- https://www.investopedia.com/terms/a/adjusted_closing_price.asp\n",
    "- https://www.investopedia.com/articles/fundamental-analysis/08/sec-forms.asp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
