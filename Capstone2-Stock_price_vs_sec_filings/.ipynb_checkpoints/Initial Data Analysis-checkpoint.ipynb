{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking out loud, at the beginning:\n",
    "- Wow these files are big, I'm going to need to pare down the data probably a lot.\n",
    "- First going to be looking for \"what's the most complete data that I have for what span of time\"?\n",
    "- What data do I actually want to focus on? (record some research on how, broadly, one comes up with a company valuation that could be comparable to market capitalization)\n",
    "- I probably should focus my market data gathering on the day after each SEC filing is released, assuming that will be the day that stock price movements are most influenced by the filing information rather than other news or factors.\n",
    "- My core question so far is, are market prices correlated with valuation information in some fashion?\n",
    "    - If so, which valuation information is most strongly correlated?\n",
    "    - What, if any, is the relationship between stock price/market capitalization and the business's balance sheet?\n",
    "    - Do any of these data include attempts to put a value on intangibles such as goodwill/reputation?\n",
    "    - The fundamental analysis tradition focuses on ratios of price to earnings, book value, earnings growth; do the ratios that investors find acceptable change over time? Do they tend to change back (or ahead) when SEC filings are released?\n",
    "    - (Do I have time for this?) What happens in the periods between filings? Do prices maintain a range, like the moving average support/resistance bands favored by technical analysis?\n",
    "- Therefore, what am I looking for in the data I've found so far?\n",
    "    - As much history as I can assemble from both market data and SEC filings; will need to drop years where I have one but not the other.\n",
    "    - Focus on the Fortune 100, probably choose the 100 as of the most recent year I have data for.\n",
    "    - Market data at daily granularity, hopefully with open, high, low, and close prices for each day. Probably focus on dates related to the company's SEC filings.\n",
    "    - Still deciding what SEC filing information, partially I don't know what I have to work with yet, hoping for a moderately quick thumbnail of value related to assets, liabilities, and maybe intangibles/goodwill. Assuming earnings-per-share numbers also will be found here.\n",
    "    - If market data has already calculated P/E or other ratios, great, I can use those to sanity-check what I'm pulling in from SEC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First crack at a plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Open each dataset (I have 4 right now) individually, probably not in code. \n",
    "- Need to see file structure and years covered. Make decision on how much history to collect/collate.\n",
    "    - Market datasets from:\n",
    "        - https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs\n",
    "            - description does not offer a begin date but last updated in 2017-Nov\n",
    "            - data appears to be in txt files by ticker: file for AAPL dates to 1984-Sep and columns are Date,Open,High,Low,Close,Volume,OpenInt\n",
    "            - there is a comment that prices in this data are adjusted for dividends AND splits (as opposed to Yahoo Finance data, which is not or not always--not sure what that means)\n",
    "        - https://www.kaggle.com/tsaustin/us-historical-stock-prices-with-earnings-data\n",
    "            - 20 years according to description; last updated 2020-Dec\n",
    "            - Data in 4 files: stock price, earnings (estimated and reported), dividends, and a summary\n",
    "    - SEC datasets from:\n",
    "        - https://www.kaggle.com/miguelaenlle/parsed-sec-10q-filings-since-2006\n",
    "            - Since 2006 according to description; Last updated 2020-Jul\n",
    "            - Notes that ~50% of data is null, which might be 'not found in filing' or 'not found by parser'.\n",
    "            - Does have lots of promising columns like commonstocksharesissued, assetscurrent, accountspayablecurrent, commonstockvalue, liabilities, liabilitiesandstockholdersequity, stockholdersequity, earningspersharebasic, netincomeloss, profitloss, costofgoodssold, filing_date, costsandexpenses, cash \n",
    "        - https://www.kaggle.com/finnhub/reported-financials\n",
    "            - 2010-2020, according to description. Last updated 2021-Mar\n",
    "            - 4 GB data!\n",
    "            - Data is in folders by year-qtr; JSON files. Will take some work, I think, to find the tickers I want.\n",
    "            \n",
    "So, I can probably get 2010-2020 reasonably complete. Maybe 2006-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import os\n",
    "import json\n",
    "#from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Open each dataset in code and examine columns.\n",
    "See if I can choose one market and one SEC dataset; do they contain the same information? Is there just a column or two from one I can add to the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a: the Finnhub SEC filings dataset\n",
    "- Hopefully the most complicated one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Value Error: Value is too big\n",
      "Value Error1: Expected object or value\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Found one!\n",
      "Files found: 44\n",
      "Problem files found: 2\n"
     ]
    }
   ],
   "source": [
    "# Note some of this code may not work as written except on my own computer, or if you download the datasets like I did.\n",
    "# Not planning to upload all this data, it's waaay too much.\n",
    "# CAUTION: walking all this data will take a long time! Over 200K files, expect 20+ min of \"busy\" before you get an answer.\n",
    "\n",
    "# For Finnhub reported-financials dataset:\n",
    "# Recurse through many YYYY.Q# subdirectories using os.walk\n",
    "# Acquire list of files containing the ticker(s) I want (define a list somewhere early!)\n",
    "# Copy those files to a \"data I'm keeping\" folder\n",
    "# Import reduced file set into a dataframe for analysis\n",
    "# I thought about defining a function, but it's pretty specific to how this one dataset is laid out, is it worth it?\n",
    "\n",
    "ticker_list = ['AAPL']\n",
    "start_dir = os.getcwd() \n",
    "\n",
    "# def get_files_with_ticker(tickers, startdir):\n",
    "files_list = []\n",
    "problem_files = []\n",
    "    #for root, subdirs, files in os.walk(startdir):\n",
    "for root, subdirs, files in os.walk(start_dir):\n",
    "    for filename in files:\n",
    "        if os.path.splitext(filename)[1] == ('.json'):\n",
    "            file_path = os.path.join(root, filename)\n",
    "            #print(file_path)\n",
    "            try:\n",
    "                df = pd.read_json(file_path) # is there a better way to read json if all I want is to check one column?\n",
    "                #print(df.head(1))\n",
    "            except ValueError as val_err:\n",
    "                #print(\"Value Error: {0}\".format(val_err))\n",
    "                problem_files.append((file_path, \"Value Error: {0}\".format(val_err)))\n",
    "            except:\n",
    "                print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "                raise\n",
    "            else:\n",
    "                #set_tickers = set(tickers)\n",
    "                set_tickers = set(ticker_list)\n",
    "                set_symbols = set(df['symbol'].unique())\n",
    "                if set_tickers.intersection(set_symbols):\n",
    "                    # print('Found one!')\n",
    "                    files_list.append(file_path)\n",
    "    #return files_list\n",
    "print('Files found: ' + str(len(files_list)))\n",
    "print('Problem files found: ' + str(len(problem_files)))\n",
    "#copy_files = get_files_with_ticker(ticker_list, start_dir)\n",
    "\n",
    "# first try at running this took 35+ minutes and produced \"value too big\" somewhere in a read_json call\n",
    "# I am grateful that Jupyter changes the tab icon to hourglass when a notebook is busy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\GitHub\\Springboard\\Capstone2-Stock_price_vs_sec_filings\\SEC-FinancialsAsReported\\2016.QTR4\\0001098009-16-000023.json\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Value is too big",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-885be19383a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfile_problem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'SEC-FinancialsAsReported\\\\2016.QTR4\\\\0001098009-16-000023.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_problem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_problem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# df = pd.read_json(file_problem, lines=True) #did not work, \"expected object or value\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# df = pd.read_json(file_problem, orient='records') #did not work, \"value is too big\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m                 )\n\u001b[0;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[0;32m    616\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    753\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"series\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 886\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m-> 1119\u001b[1;33m                 \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m             )\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Value is too big"
     ]
    }
   ],
   "source": [
    "# experimenting with the one problem file. Set lines? orient? skip because it's one file out of 200K or so?\n",
    "'''\n",
    "print(os.getcwd())\n",
    "file_problem = os.path.join(os.getcwd(),'SEC-FinancialsAsReported\\\\2016.QTR4\\\\0001098009-16-000023.json')\n",
    "print(file_problem)\n",
    "'''\n",
    "# tried a lot of variations to get path join to work by hand (as opposed to using walk). \n",
    "# The two most important seem to be start with os.getcwd() and escape any slashes in the path.\n",
    "\n",
    "'''\n",
    "df = pd.read_json(file_problem) #as done in filewalk above, \"value is too big\"\n",
    "df = pd.read_json(file_problem, lines=True) #did not work, \"expected object or value\"\n",
    "df = pd.read_json(file_problem, orient='records') #did not work, \"value is too big\"\n",
    "df = pd.read_json(file_problem, orient='split') #did not work, \"value is too big\"\n",
    "df = pd.read_json(file_problem, orient='index') #did not work, \"value is too big\"\n",
    "df = pd.read_json(file_problem, orient='columns') #did not work, \"value is too big\"\n",
    "df = pd.read_json(file_problem, orient='values') #did not work, \"value is too big\"\n",
    "print(df.head)\n",
    "'''\n",
    "# nothing is working so far, skip this damn file. Opened in Notepad++ and its ticker is 'CWNM' \n",
    "# which I don't think is on my list anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, to see what I have for my ticker(s) in finnhub files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               label  \\\n",
      "0                    Long-term marketable securities   \n",
      "1  Tangible assets that are held by an entity for...   \n",
      "2                                           Goodwill   \n",
      "3                    Acquired intangible assets, net   \n",
      "4                                       Other assets   \n",
      "\n",
      "                                             concept unit        value  \\\n",
      "0               AvailableForSaleSecuritiesNoncurrent  usd  18549000000   \n",
      "1  aapl:PropertyPlantAndEquipmentAndCapitalizedSo...  usd   3504000000   \n",
      "2                                           Goodwill  usd    480000000   \n",
      "3               IntangibleAssetsNetExcludingGoodwill  usd    263000000   \n",
      "4                              OtherAssetsNoncurrent  usd   1925000000   \n",
      "\n",
      "    startDate     endDate  year quarter symbol  \n",
      "0  2009-09-27  2010-03-27  2010      Q2   AAPL  \n",
      "1  2009-09-27  2010-03-27  2010      Q2   AAPL  \n",
      "2  2009-09-27  2010-03-27  2010      Q2   AAPL  \n",
      "3  2009-09-27  2010-03-27  2010      Q2   AAPL  \n",
      "4  2009-09-27  2010-03-27  2010      Q2   AAPL  \n",
      "                                               label  \\\n",
      "0           Depreciation, amortization and accretion   \n",
      "1                   Stock-based compensation expense   \n",
      "2                        Deferred income tax expense   \n",
      "3  Loss on disposition of property, plant and equ...   \n",
      "4                           Accounts receivable, net   \n",
      "\n",
      "                                   concept unit      value   startDate  \\\n",
      "0  DepreciationAmortizationAndAccretionNet  usd  425000000  2009-09-27   \n",
      "1                   ShareBasedCompensation  usd  436000000  2009-09-27   \n",
      "2          DeferredIncomeTaxExpenseBenefit  usd  893000000  2009-09-27   \n",
      "3   GainLossOnSaleOfPropertyPlantEquipment  usd   -9000000  2009-09-27   \n",
      "4     IncreaseDecreaseInAccountsReceivable  usd -482000000  2009-09-27   \n",
      "\n",
      "      endDate  year quarter symbol  \n",
      "0  2010-03-27  2010      Q2   AAPL  \n",
      "1  2010-03-27  2010      Q2   AAPL  \n",
      "2  2010-03-27  2010      Q2   AAPL  \n",
      "3  2010-03-27  2010      Q2   AAPL  \n",
      "4  2010-03-27  2010      Q2   AAPL  \n",
      "                                 label  \\\n",
      "0            Earnings Per Share, Basic   \n",
      "1          Earnings Per Share, Diluted   \n",
      "2             Research and development   \n",
      "3  Selling, general and administrative   \n",
      "4             Total operating expenses   \n",
      "\n",
      "                                  concept        unit         value  \\\n",
      "0                   EarningsPerShareBasic  usd/shares  7.120000e+00   \n",
      "1                 EarningsPerShareDiluted  usd/shares  7.000000e+00   \n",
      "2           ResearchAndDevelopmentExpense         usd  8.240000e+08   \n",
      "3  SellingGeneralAndAdministrativeExpense         usd  2.508000e+09   \n",
      "4                       OperatingExpenses         usd  3.332000e+09   \n",
      "\n",
      "    startDate     endDate  year quarter symbol  \n",
      "0  2009-09-27  2010-03-27  2010      Q2   AAPL  \n",
      "1  2009-09-27  2010-03-27  2010      Q2   AAPL  \n",
      "2  2009-09-27  2010-03-27  2010      Q2   AAPL  \n",
      "3  2009-09-27  2010-03-27  2010      Q2   AAPL  \n",
      "4  2009-09-27  2010-03-27  2010      Q2   AAPL  \n"
     ]
    }
   ],
   "source": [
    "# concat/map solution found at https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe\n",
    "#finnhub_df = pd.concat(map(pd.read_json, files_list))\n",
    "# not working well due to nested JSON, I think...\n",
    "\n",
    "# json_normalize found at https://stackoverflow.com/questions/20638006/convert-list-of-dictionaries-to-a-pandas-dataframe\n",
    "# convert to dict clue found at https://stackoverflow.com/questions/55710154/python-json-normalize-string-indices-must-be-integers-error\n",
    "# nested JSON clue 1: https://stackoverflow.com/questions/47341519/json-normalize-for-dicts-within-dicts\n",
    "# nested JSON clue 2: https://www.kaggle.com/jboysen/quick-tutorial-flatten-nested-json-in-pandas/notebook\n",
    "\n",
    "for i in range(len(files_list)):\n",
    "    with open(files_list[i]) as f:\n",
    "        d = json.load(f)\n",
    "    if i == 0:\n",
    "        balsheet_df = pd.json_normalize(d, record_path=['data','bs'], meta=['startDate','endDate','year','quarter','symbol'])\n",
    "        cashflow_df = pd.json_normalize(d, record_path=['data','cf'], meta=['startDate','endDate','year','quarter','symbol'])\n",
    "        income_df = pd.json_normalize(d, record_path=['data','ic'], meta=['startDate','endDate','year','quarter','symbol'])\n",
    "    else:\n",
    "        balsheet_df.append(pd.json_normalize(d, record_path=['data','bs'], meta=['startDate','endDate','year','quarter','symbol']))\n",
    "        cashflow_df.append(pd.json_normalize(d, record_path=['data','cf'], meta=['startDate','endDate','year','quarter','symbol']))\n",
    "        income_df.append(pd.json_normalize(d, record_path=['data','ic'], meta=['startDate','endDate','year','quarter','symbol']))\n",
    "\n",
    "print(balsheet_df.head())\n",
    "print(cashflow_df.head())\n",
    "print(income_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some things I needed to find out:\n",
    "- Why are all these reports split into records called \"bs/cf/ic\"? What are those abbreviations for?\n",
    "- how about... **B**alance **S**heet, **C**ash **F**low, **I**n**C**ome statement? https://www.sec.gov/oiea/reportspubs/investor-publications/beginners-guide-to-financial-statements.html\n",
    "- I want to see more of what's in those \"data\" fields: learning how to unpack nested JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([{'label': 'Long-term marketable securities', 'concept': 'AvailableForSaleSecuritiesNoncurrent', 'unit': 'usd', 'value': 18549000000}, {'label': 'Tangible assets that are held by an entity for use in the production or supply of goods and services, for rental to others, or for administrative purposes and that are expected to provide economic benefit for more than one year; net of accumulated depreciation. Examples include land, buildings, and production equipment. Also includes software programs or applications for internal use (that is, not to be sold, leased or otherwise marketed to others) that qualify for capitalization.', 'concept': 'aapl:PropertyPlantAndEquipmentAndCapitalizedSoftwareNet', 'unit': 'usd', 'value': 3504000000}, {'label': 'Goodwill', 'concept': 'Goodwill', 'unit': 'usd', 'value': 480000000}, {'label': 'Acquired intangible assets, net', 'concept': 'IntangibleAssetsNetExcludingGoodwill', 'unit': 'usd', 'value': 263000000}, {'label': 'Other assets', 'concept': 'OtherAssetsNoncurrent', 'unit': 'usd', 'value': 1925000000}, {'label': 'Total assets', 'concept': 'Assets', 'unit': 'usd', 'value': 57057000000}, {'label': 'Cash and cash equivalents', 'concept': 'CashAndCashEquivalentsAtCarryingValue', 'unit': 'usd', 'value': 10018000000}, {'label': 'Short-term marketable securities', 'concept': 'AvailableForSaleSecuritiesCurrent', 'unit': 'usd', 'value': 13137000000}, {'label': 'Accounts receivable, less allowances of $57 and $52, respectively', 'concept': 'AccountsReceivableNetCurrent', 'unit': 'usd', 'value': 2886000000}, {'label': 'Inventories', 'concept': 'InventoryNet', 'unit': 'usd', 'value': 638000000}, {'label': 'Deferred tax assets', 'concept': 'DeferredTaxAssetsNetCurrent', 'unit': 'usd', 'value': 1142000000}, {'label': 'Other current assets', 'concept': 'OtherAssetsCurrent', 'unit': 'usd', 'value': 4515000000}, {'label': 'Total current assets', 'concept': 'AssetsCurrent', 'unit': 'usd', 'value': 32336000000}, {'label': 'Deferred revenue - non-current', 'concept': 'DeferredRevenueNoncurrent', 'unit': 'usd', 'value': 941000000}, {'label': 'Other non-current liabilities', 'concept': 'OtherLiabilitiesNoncurrent', 'unit': 'usd', 'value': 4539000000}, {'label': 'Total liabilities', 'concept': 'Liabilities', 'unit': 'usd', 'value': 17709000000}, {'label': \"Total liabilities and shareholders' equity\", 'concept': 'LiabilitiesAndStockholdersEquity', 'unit': 'usd', 'value': 57057000000}, {'label': 'Accounts Payable, Current', 'concept': 'AccountsPayableCurrent', 'unit': 'usd', 'value': 5666000000}, {'label': 'Accrued expenses', 'concept': 'AccruedLiabilitiesCurrent', 'unit': 'usd', 'value': 4021000000}, {'label': 'Deferred Revenue, Current', 'concept': 'DeferredRevenueCurrent', 'unit': 'usd', 'value': 2542000000}, {'label': 'Total current liabilities', 'concept': 'LiabilitiesCurrent', 'unit': 'usd', 'value': 12229000000}, {'label': 'Common stock, no par value; 1,800,000,000 shares authorized; 909,635,811 and 899,805,500 shares issued and outstanding, respectively', 'concept': 'CommonStockValue', 'unit': 'usd', 'value': 9553000000}, {'label': 'Retained earnings', 'concept': 'RetainedEarningsAccumulatedDeficit', 'unit': 'usd', 'value': 29670000000}, {'label': 'Accumulated other comprehensive income', 'concept': 'AccumulatedOtherComprehensiveIncomeLossNetOfTax', 'unit': 'usd', 'value': 125000000}, {'label': \"Total shareholders' equity\", 'concept': 'StockholdersEquity', 'unit': 'usd', 'value': 39348000000}])]\n"
     ]
    }
   ],
   "source": [
    "# figure out how to pull this data apart into each category and get \"data\" out of being sub-dictionaries\n",
    "#print(finnhub_df['data'].head(1))\n",
    "#print(finnhub_df['data'].head(1).values)\n",
    "bs_df = pd.json_normalize(finnhub_df['data'].head(1).values)\n",
    "print(bs_df[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Choose 100 companies and pull their data into new files to reduce data sizes. \n",
    "Let's try the 2020 Fortune 100.\n",
    "Data retrieved from https://fortune.com/fortune500/2020/search/?rank=asc\n",
    "Spent a bunch of time massaging the text as copied back into tabular format. Spent a bunch more time going through each company's profile page to pull out ticker symbols. Hopefully an employer would have a better way to access the data, but it's good enough for this project. They want \\$500 or more for a data file and don't talk about having an API that I can see.\n",
    "\n",
    "- Some ticker notes:\n",
    "    - There are a bunch of companies (especially mutual insurance companies) without tickers, so I'm going to end up with less than 100 companies. Expand search or leave it?\n",
    "    - Will I need to look out for name changes (esp ticker changes)?\n",
    "    - Albertsons has a ticker but IPO was last year sometime so may not have data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Begin EDA, look for cleaning needs. \n",
    "- Figure out how best to pull only the SEC-related dates of market data, assuming that dates will generally be different for each company.\n",
    "- Calculate (can I sanity-check somehow?) fundamental analysis ratios: P/E, P/E/G, price/book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other tasks as I think of them while exploring!\n",
    "- Do I have data that shows market capitalization? Or shares outstanding, so I can calculate?\n",
    "- How does one adjust prices for dividends, splits? What price do I need to use to calculate ratios, market cap, etc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions for later:\n",
    "- Once I produce something with this, should I share it on Kaggle? If I've combined datasets, can I link my code to both of them?\n",
    "- Can or should I try to confirm that Yahoo Finance is a bad place to get stock data from? (see notes on first market dataset above)\n",
    "- Suddenly, after importing a couple hundred thousand files, GitHub desktop can't find my repository... it can find sub-repositories like the ones I cloned from other projects but not my main Springboard folder.\n",
    "- How do I make Pandas read from somewhere other than the working directory?\n",
    "- How do I pull out the data I want from these JSON files and save for later? As dataframe, csv, something else?\n",
    "- Any ideas how I figure out what the 'bs/cf/ic' categories (row labels) are in the JSON files?\n",
    "- Can I pull additional company data from the Fortune site? Should I? https://fortune.com/company/stonex-group/fortune500/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research:\n",
    "- https://www.investopedia.com/terms/a/adjusted_closing_price.asp\n",
    "- https://www.investopedia.com/articles/fundamental-analysis/08/sec-forms.asp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
